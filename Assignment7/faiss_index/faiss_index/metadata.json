[{"text": "Building Effective AI Agents \\ Anthropic Skip to main content Skip to footer Claude API Solutions Research Commitments Learn News Try Claude Engineering at Anthropic Building effective agents Published Dec 19, 2024 We've worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple, composable patterns rather than complex frameworks. Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns. In this post, we share what we\u2019ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents. What are agents? \"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as agentic systems , but draw an important architectural distinction between workflows and agents : Workflows are systems where LLMs and tools are orchestrated through predefined code paths.", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-20T23:22:41.357596", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_0", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as agentic systems , but draw an important architectural distinction between workflows and agents : Workflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents , on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. Below, we will explore both types of agentic systems in detail. In Appendix 1 (\u201cAgents in Practice\u201d), we describe two domains where customers have found particular value in using these kinds of systems. When (and when not) to use agents When building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense. When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-20T23:22:41.357620", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_150", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "better task performance, and you should consider when this tradeoff makes sense. When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough. When and how to use frameworks There are many frameworks that make agentic systems easier to implement, including: LangGraph from LangChain; Amazon Bedrock's AI Agent framework ; Rivet , a drag and drop GUI LLM workflow builder; and Vellum , another GUI tool for building and testing complex workflows. These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts \u200b\u200band responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice. We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-20T23:22:41.357626", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_300", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "They can also make it tempting to add complexity when a simpler setup would suffice. We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error. See our cookbook for some sample implementations. Building blocks, workflows, and agents In this section, we\u2019ll explore the common patterns for agentic systems we\u2019ve seen in production. We'll start with our foundational building block\u2014the augmented LLM\u2014and progressively increase complexity, from simple compositional workflows to autonomous agents. Building block: The augmented LLM The basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities\u2014generating their own search queries, selecting appropriate tools, and determining what information to retain. The augmented LLM We recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-20T23:22:41.357630", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_450", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "information to retain. The augmented LLM We recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released Model Context Protocol , which allows developers to integrate with a growing ecosystem of third-party tools with a simple client implementation . For the remainder of this post, we'll assume each LLM call has access to these augmented capabilities. Workflow: Prompt chaining Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate\u201d in the diagram below) on any intermediate steps to ensure that the process is still on track. The prompt chaining workflow When to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task. Examples where prompt chaining is useful: Generating Marketing copy, then translating it into a different language.", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-20T23:22:41.357635", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_600", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task. Examples where prompt chaining is useful: Generating Marketing copy, then translating it into a different language. Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline. Workflow: Routing Routing classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs. The routing workflow When to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm. Examples where routing is useful: Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools. Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-20T23:22:41.357639", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_750", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "traditional classification model/algorithm. Examples where routing is useful: Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools. Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed. Workflow: Parallelization LLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning : Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs. The parallelization workflow When to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect. Examples where parallelization is useful: Sectioning : Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-20T23:22:41.357642", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_900", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "LLM call, allowing focused attention on each specific aspect. Examples where parallelization is useful: Sectioning : Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response. Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model\u2019s performance on a given prompt. Voting : Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem. Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives. Workflow: Orchestrator-workers In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results. The orchestrator-workers workflow When to use this workflow: This workflow is well-suited for complex tasks where you can\u2019t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\u2019s topographically similar, the key", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-20T23:22:41.357647", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_1050", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "use this workflow: This workflow is well-suited for complex tasks where you can\u2019t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\u2019s topographically similar, the key difference from parallelization is its flexibility\u2014subtasks aren't pre-defined, but determined by the orchestrator based on the specific input. Example where orchestrator-workers is useful: Coding products that make complex changes to multiple files each time. Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information. Workflow: Evaluator-optimizer In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop. The evaluator-optimizer workflow When to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document. Examples where evaluator-optimizer is useful:", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-20T23:22:41.357651", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_1200", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document. Examples where evaluator-optimizer is useful: Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques. Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted. Agents Agents are emerging in production as LLMs mature in key capabilities\u2014understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain \u201cground truth\u201d from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-20T23:22:41.357655", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_1350", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "returning to the human for further information or judgement. During execution, it's crucial for the agents to gain \u201cground truth\u201d from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it\u2019s also common to include stopping conditions (such as a maximum number of iterations) to maintain control. Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 (\"Prompt Engineering your Tools\"). Autonomous agent When to use agents: Agents can be used for open-ended problems where it\u2019s difficult or impossible to predict the required number of steps, and where you can\u2019t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments. The autonomous nature of agents means higher costs, and", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-20T23:22:41.357659", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_1500", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "number of steps, and where you can\u2019t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments. The autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails. Examples where agents are useful: The following examples are from our own implementations: A coding Agent to resolve SWE-bench tasks , which involve edits to many files based on a task description; Our \u201ccomputer use\u201d reference implementation , where Claude uses a computer to accomplish tasks. High-level flow of a coding agent Combining and customizing these patterns These building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity only when it demonstrably improves outcomes. Summary Success in the LLM space isn't about building the most sophisticated system. It's about building the right system for your needs. Start with simple prompts, optimize them", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 1650, "end": 1849}, "timestamp": "2025-04-20T23:22:41.357663", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_1650", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity only when it demonstrably improves outcomes. Summary Success in the LLM space isn't about building the most sophisticated system. It's about building the right system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short. When implementing agents, we try to follow three core principles: Maintain simplicity in your agent's design. Prioritize transparency by explicitly showing the agent\u2019s planning steps. Carefully craft your agent-computer interface (ACI) through thorough tool documentation and testing . Frameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users. Acknowledgements Written by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful. Appendix 1: Agents in practice Our work with customers has revealed two particularly promising applications for AI agents that demonstrate the", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 1800, "end": 1999}, "timestamp": "2025-04-20T23:22:41.357667", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_1800", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful. Appendix 1: Agents in practice Our work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight. A. Customer support Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because: Support interactions naturally follow a conversation flow while requiring access to external information and actions; Tools can be integrated to pull customer data, order history, and knowledge base articles; Actions such as issuing refunds or updating tickets can be handled programmatically; and Success can be clearly measured through user-defined resolutions. Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness. B. Coding agents The software development space has shown remarkable potential for LLM features,", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 1950, "end": 2149}, "timestamp": "2025-04-20T23:22:41.357671", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_1950", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "handled programmatically; and Success can be clearly measured through user-defined resolutions. Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness. B. Coding agents The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because: Code solutions are verifiable through automated tests; Agents can iterate on solutions using test results as feedback; The problem space is well-defined and structured; and Output quality can be measured objectively. In our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements. Appendix 2: Prompt engineering your tools No matter which agentic system you're building, tools will likely be an important part of your agent. Tools enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a tool use block in the API response if it plans to invoke a", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 2100, "end": 2299}, "timestamp": "2025-04-20T23:22:41.357679", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_2100", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "tools will likely be an important part of your agent. Tools enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a tool use block in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools. There are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes. Our suggestions for deciding on tool formats are the following: Give the model enough", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 2250, "end": 2449}, "timestamp": "2025-04-20T23:22:41.357684", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_2250", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes. Our suggestions for deciding on tool formats are the following: Give the model enough tokens to \"think\" before it writes itself into a corner. Keep the format close to what the model has seen naturally occurring in text on the internet. Make sure there's no formatting \"overhead\" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes. One rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good agent -computer interfaces (ACI). Here are some thoughts on how to do so: Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it\u2019s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 2400, "end": 2599}, "timestamp": "2025-04-20T23:22:41.357689", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_2400", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it\u2019s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools. How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools. Test how the model uses your tools: Run many example inputs in our workbench to see what mistakes the model makes, and iterate. Poka-yoke your tools. Change the arguments so that it is harder to make mistakes. While building our agent for SWE-bench , we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths\u2014and we found that the model used this method flawlessly. Product Claude overview Claude team plan Claude enterprise plan Claude", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 2550, "end": 2749}, "timestamp": "2025-04-20T23:22:41.357693", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_2550", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths\u2014and we found that the model used this method flawlessly. Product Claude overview Claude team plan Claude enterprise plan Claude education plan Download Claude apps Claude.ai pricing plans Claude.ai login API Platform API overview Developer docs Claude in Amazon Bedrock Claude on Google Cloud's Vertex AI Pricing Console login Research Research overview Economic Index Claude models Claude 3.7 Sonnet Claude 3.5 Haiku Claude 3 Opus Commitments Transparency Responsible scaling policy Security and compliance Solutions AI agents Coding Customer support Learning resources News Customer stories Engineering at Anthropic Anthropic Academy Company About us Careers Help and security Status Availability Support center Terms and policies Privacy choices Privacy policy Responsible disclosure policy Terms of service - consumer Terms of service - commercial Usage policy \u00a9 2025 Anthropic PBC", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 2700, "end": 2855}, "timestamp": "2025-04-20T23:22:41.357696", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_2700", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "Usage policy \u00a9 2025 Anthropic PBC", "url": "https://www.anthropic.com/engineering/building-effective-agents", "position": {"start": 2850, "end": 2855}, "timestamp": "2025-04-20T23:22:41.357697", "chunk_id": "https_www_anthropic_com_engineering_building-effective-agents_2850", "title": "Building Effective AI Agents \\ Anthropic"}, {"text": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium Open in app Write Get unlimited access to the best of Medium for less than $1/week. Become a member Become a member The Paradigm Shift in Decision Making with Foundation Agents Bijit Ghosh \u00b7 Follow 13 min read \u00b7 Jun 2, 2024 6 Listen Share More Looking to hide highlights? You can now hide them from the \u201c\u2022\u2022\u2022\u201d menu. Okay, got it Introduction Let\u2019s talk about something that\u2019s been causing quite a buzz in the AI world lately \u2014 foundation agents and their potential to revolutionize decision-making as we know it. Now, I know what you might be thinking, \u201canother day, another AI breakthrough, ho-hum.\u201d But trust me, this one\u2019s a game-changer, and it\u2019s worth your attention. If you\u2019ve been following the advancements in AI and AI Agents, you\u2019re probably familiar with large language models (LLMs) like GPT-4, which can tackle a wide range of tasks with minimal fine-tuning. These language juggernauts have shown us just how versatile and adaptable AI can be. But here\u2019s the thing \u2014 when it comes to decision-making in complex, ever-changing environments, even the mightiest LLMs can struggle to put", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-20T23:24:04.543185", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_0", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "(LLMs) like GPT-4, which can tackle a wide range of tasks with minimal fine-tuning. These language juggernauts have shown us just how versatile and adaptable AI can be. But here\u2019s the thing \u2014 when it comes to decision-making in complex, ever-changing environments, even the mightiest LLMs can struggle to put all the pieces together efficiently. That\u2019s where foundation agents come into play. Think of them as the conductors that can orchestrate the collective intelligence of multiple language models, both large and small, to make well-informed decisions that consider all angles. It\u2019s like having a team of specialized experts, each with their unique knowledge and skills, but instead of working in silos, they\u2019re collaborating seamlessly under the guidance of a highly adaptable coordinator. But foundation agents aren\u2019t just about efficiency; they represent a paradigm shift in how we approach decision-making in AI. By fostering cross-pollination of knowledge and enabling continuous learning, they have the potential to uncover novel solutions and push the boundaries of what\u2019s possible in various domains, from healthcare and finance to urban planning and environmental conservation. Now, I know what you\u2019re thinking, \u201cThis all sounds great, but how do we actually create these foundation agents?\u201d Well, buckle up,", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-20T23:24:04.543199", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_150", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "they have the potential to uncover novel solutions and push the boundaries of what\u2019s possible in various domains, from healthcare and finance to urban planning and environmental conservation. Now, I know what you\u2019re thinking, \u201cThis all sounds great, but how do we actually create these foundation agents?\u201d Well, buckle up, because we\u2019re about to embark on a journey through the roadmap, architectural considerations that will bring this concept to life. So, whether you\u2019re a tech enthusiast, a decision-maker in your industry, or just someone who\u2019s fascinated by the latest AI breakthroughs, get ready to dive deep into the world of foundation agents. Trust me; it\u2019s a wild ride you don\u2019t want to miss! The Problem: Challenges in Decision-Making Agents Decision-making agents are tasked with operating in complex, dynamic environments, where they must perceive, process, and act upon a vast array of inputs and stimuli. Traditionally, these agents have been developed through specialized training processes, tailored to specific tasks or domains. However, this approach often leads to several significant challenges: Sample Inefficiency: Specialized training processes require vast amounts of data and computational resources, making them sample-inefficient and resource-intensive. Lack of Generalization: Agents trained for specific tasks or domains struggle to generalize", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-20T23:24:04.543204", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_300", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "through specialized training processes, tailored to specific tasks or domains. However, this approach often leads to several significant challenges: Sample Inefficiency: Specialized training processes require vast amounts of data and computational resources, making them sample-inefficient and resource-intensive. Lack of Generalization: Agents trained for specific tasks or domains struggle to generalize and adapt to new scenarios or environments, limiting their versatility and real-world applicability. Fragmented Knowledge: Different agents are trained independently, leading to a fragmentation of knowledge and inefficient information sharing, hindering their ability to leverage collective intelligence. Complex Integration: Integrating multiple specialized agents into a cohesive system can be a daunting task, often requiring intricate coordination and communication mechanisms. Scalability Limitations: As the complexity of tasks and environments increases, the scalability of traditional agent learning approaches becomes a significant bottleneck. The Solution: Foundation Agents Foundation agents represent a paradigm shift in agent learning, drawing inspiration from the success of foundation models like LLMs. These agents are designed to rapidly adapt to new tasks, leveraging the collective knowledge and capabilities of multiple language models, both large and small, in a seamless and efficient manner. The key characteristics of foundation agents include: Rapid Adaptability: Similar to LLMs, foundation agents possess the ability", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-20T23:24:04.543209", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_450", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "models like LLMs. These agents are designed to rapidly adapt to new tasks, leveraging the collective knowledge and capabilities of multiple language models, both large and small, in a seamless and efficient manner. The key characteristics of foundation agents include: Rapid Adaptability: Similar to LLMs, foundation agents possess the ability to rapidly adapt to new tasks with minimal fine-tuning, enabling them to tackle a wide range of decision-making scenarios. Collective Intelligence: Foundation agents can leverage the collective knowledge and capabilities of multiple language models, both large and small, by abstracting and distributing the workload across these models. Scalable Architecture: The underlying architecture of foundation agents is designed to be scalable, allowing for the integration of additional language models and the seamless distribution of computational tasks. Efficient Resource Utilization: By leveraging the strengths of different language models and distributing tasks accordingly, foundation agents can optimize resource utilization and enhance overall efficiency. Continuous Learning: Foundation agents are designed to learn and evolve continuously, incorporating new knowledge and adapting to changing environments and requirements. The Roadmap: Creating Foundation Agents The development of foundation agents involves a multifaceted approach, spanning data collection, pretraining techniques, and the alignment of knowledge and values with existing LLMs.", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-20T23:24:04.543212", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_600", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "Continuous Learning: Foundation agents are designed to learn and evolve continuously, incorporating new knowledge and adapting to changing environments and requirements. The Roadmap: Creating Foundation Agents The development of foundation agents involves a multifaceted approach, spanning data collection, pretraining techniques, and the alignment of knowledge and values with existing LLMs. Here\u2019s a roadmap outlining the key steps in this process: Data Collection and Generation: Collect large-scale interactive datasets spanning diverse domains and scenarios. Employ techniques such as web scraping, crowdsourcing, and synthetic data generation to build comprehensive and diverse datasets. Ensure the data captures the complexities of decision-making processes, including perception, memory, and reasoning. Self-Supervised Pretraining: Leverage self-supervised learning techniques to pretrain foundation agents on the collected interactive datasets. Explore techniques such as contrastive learning, masked language modeling, and Next Sentence Prediction (NSP) to enable the agents to learn from unlabeled data. Implement multi-task learning approaches to enable the agents to learn multiple decision-making tasks simultaneously. Adaptation and Fine-tuning: Develop adaptation and fine-tuning strategies to enable foundation agents to rapidly specialize in specific tasks or domains. Explore techniques such as prompt-based learning, few-shot learning, and transfer learning to leverage the knowledge acquired during pretraining. Implement continual learning approaches to enable", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-20T23:24:04.543217", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_750", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "to learn multiple decision-making tasks simultaneously. Adaptation and Fine-tuning: Develop adaptation and fine-tuning strategies to enable foundation agents to rapidly specialize in specific tasks or domains. Explore techniques such as prompt-based learning, few-shot learning, and transfer learning to leverage the knowledge acquired during pretraining. Implement continual learning approaches to enable the agents to continuously adapt and learn from new data and experiences. Knowledge and Value Alignment: Align the knowledge and values of foundation agents with those of existing LLMs and other language models. Leverage techniques like knowledge distillation, model compression, and value alignment to ensure consistency and coherence across different models. Implement safeguards and ethical principles to ensure the agents operate within predefined boundaries and adhere to societal norms and values. Multi-Model Integration: Develop architectures and protocols for seamless integration and communication between foundation agents and other language models (LLMs & SLMs). Implement load balancing and task distribution mechanisms to optimize resource utilization and enhance overall efficiency. Explore techniques for knowledge sharing and transfer between different models, fostering collective intelligence and enabling continuous learning. Architectural Considerations and Solutions Designing an efficient and scalable architecture for foundation agents is a critical aspect of this paradigm shift. Here are some key architectural", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-20T23:24:04.543221", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_900", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "optimize resource utilization and enhance overall efficiency. Explore techniques for knowledge sharing and transfer between different models, fostering collective intelligence and enabling continuous learning. Architectural Considerations and Solutions Designing an efficient and scalable architecture for foundation agents is a critical aspect of this paradigm shift. Here are some key architectural considerations and potential solutions: Modular and Extensible Design: Implement a modular architecture that allows for the seamless integration of new language models and components. Develop standardized interfaces and communication protocols to facilitate interoperability between different models and components. Ensure the architecture is extensible and can accommodate future advancements and requirements without significant modifications. Distributed Computing and Parallelization: Leverage distributed computing and parallelization techniques to enhance computational efficiency and scalability. Implement load balancing mechanisms to distribute computational tasks across multiple language models and hardware resources. Explore techniques such as model parallelism and data parallelism to optimize resource utilization and accelerate training and inference processes. Efficient Communication and Coordination: Develop efficient communication protocols and mechanisms for coordinating the interactions between different language models and components. Implement techniques like message queuing, event-driven architectures, and publish-subscribe patterns to enable asynchronous and decoupled communication. Explore techniques for conflict resolution and consensus building when multiple models", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-20T23:24:04.543225", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_1050", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "and inference processes. Efficient Communication and Coordination: Develop efficient communication protocols and mechanisms for coordinating the interactions between different language models and components. Implement techniques like message queuing, event-driven architectures, and publish-subscribe patterns to enable asynchronous and decoupled communication. Explore techniques for conflict resolution and consensus building when multiple models provide conflicting outputs or recommendations. Heterogeneous Hardware Support: Design the architecture to support a wide range of hardware platforms, including GPUs, TPUs, and specialized AI accelerators. Implement hardware-aware optimizations and leverage techniques like model quantization and pruning to optimize performance across different hardware platforms. Explore techniques for dynamic resource allocation and load balancing across heterogeneous hardware resources. Security and Privacy Considerations: Implement robust security measures to protect the system from external threats and unauthorized access. Ensure data privacy and comply with relevant regulations by implementing techniques such as differential privacy, secure multi-party computation, and homomorphic encryption. Develop mechanisms for auditing and monitoring the system to detect and mitigate potential vulnerabilities or misuse. Working with Dynamic Knowledge Graphs and Beyond One of the key strengths of foundation agents lies in their ability to seamlessly integrate and leverage various knowledge sources, including dynamic knowledge graphs and other structured data repositories. This versatility", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-20T23:24:04.543228", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_1200", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "and monitoring the system to detect and mitigate potential vulnerabilities or misuse. Working with Dynamic Knowledge Graphs and Beyond One of the key strengths of foundation agents lies in their ability to seamlessly integrate and leverage various knowledge sources, including dynamic knowledge graphs and other structured data repositories. This versatility allows them to tap into a vast wealth of information, enhancing their decision-making capabilities and adaptability. Dynamic Knowledge Graphs Knowledge graphs have emerged as powerful tools for representing and organizing complex relationships between entities, concepts, and ideas. By incorporating dynamic knowledge graphs into the foundation agent ecosystem, these agents can access and reason over vast amounts of interconnected information in real-time. The dynamic nature of these knowledge graphs is crucial, as they can continuously evolve and adapt as new information becomes available. Foundation agents can leverage their continuous learning capabilities to seamlessly incorporate these updates, ensuring that their decision-making processes are always informed by the latest relevant knowledge. Integrating Structured Data Sources Beyond knowledge graphs, foundation agents can also integrate and reason over a wide range of structured data sources, such as databases, data warehouses, and data lakes. These structured repositories often contain valuable information that can inform decision-making processes", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-20T23:24:04.543232", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_1350", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "are always informed by the latest relevant knowledge. Integrating Structured Data Sources Beyond knowledge graphs, foundation agents can also integrate and reason over a wide range of structured data sources, such as databases, data warehouses, and data lakes. These structured repositories often contain valuable information that can inform decision-making processes across various industries and domains. For instance, a foundation agent tasked with optimizing supply chain operations could tap into structured data sources containing real-time inventory levels, transportation logistics, demand forecasts, and supplier information. By synthesizing these diverse data streams, the agent could make informed decisions about inventory management, route planning, and resource allocation, enhancing overall supply chain efficiency. Multimodal Knowledge Representation To truly unlock the full potential of foundation agents, their knowledge representation capabilities must extend beyond textual data and structured sources. Multimodal knowledge representation, which encompasses various data modalities such as images, videos, audio, and sensor data, is crucial for operating in real-world environments. To achieve this, foundation agents will need to integrate cutting-edge techniques from fields like computer vision, speech recognition, and multimodal learning, enabling them to seamlessly process and reason over diverse data formats. Collaborative Knowledge Sharing One of the key advantages of foundation agents is their", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-20T23:24:04.543236", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_1500", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "for operating in real-world environments. To achieve this, foundation agents will need to integrate cutting-edge techniques from fields like computer vision, speech recognition, and multimodal learning, enabling them to seamlessly process and reason over diverse data formats. Collaborative Knowledge Sharing One of the key advantages of foundation agents is their ability to foster collaboration and knowledge sharing among various language models and components. By implementing efficient communication protocols and mechanisms, these agents can facilitate the free flow of information and insights across different knowledge sources. This collaborative knowledge sharing can take many forms, such as knowledge distillation, where smaller models can learn from the knowledge encapsulated in larger models, or transfer learning, where knowledge gained in one domain can be applied to related tasks or environments. Furthermore, foundation agents can act as knowledge brokers, connecting different components and facilitating the exchange of relevant information. This approach not only enhances the collective intelligence of the system but also promotes continuous learning and adaptation, as new knowledge and insights are constantly being integrated and disseminated. The integration of dynamic knowledge graphs, structured data sources, multimodal knowledge representation, and collaborative knowledge sharing mechanisms is crucial for unlocking the full potential of foundation agents.", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 1650, "end": 1849}, "timestamp": "2025-04-20T23:24:04.543240", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_1650", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "collective intelligence of the system but also promotes continuous learning and adaptation, as new knowledge and insights are constantly being integrated and disseminated. The integration of dynamic knowledge graphs, structured data sources, multimodal knowledge representation, and collaborative knowledge sharing mechanisms is crucial for unlocking the full potential of foundation agents. By seamlessly bridging these diverse knowledge sources, foundation agents can operate at the cutting edge of decision-making, leveraging the collective wisdom of multiple models and data streams to drive innovation and informed decision-making across various domains. Efficient Value Proposition and Future Impact The introduction of foundation agents as a paradigm shift in decision-making has the potential to revolutionize various industries and domains, delivering substantial value and impact. Here are some key aspects of the value proposition and future impact of foundation agents: Increased Efficiency and Productivity: Foundation agents\u2019 ability to rapidly adapt to new tasks and leverage collective intelligence can significantly enhance operational efficiency and productivity across various industries. By abstracting and distributing computational tasks across multiple language models, foundation agents can optimize resource utilization and accelerate decision-making processes. Improved Decision-Making Quality: The collective knowledge and capabilities of multiple language models, combined with the agents\u2019 ability to learn and evolve", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 1800, "end": 1999}, "timestamp": "2025-04-20T23:24:04.543244", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_1800", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "significantly enhance operational efficiency and productivity across various industries. By abstracting and distributing computational tasks across multiple language models, foundation agents can optimize resource utilization and accelerate decision-making processes. Improved Decision-Making Quality: The collective knowledge and capabilities of multiple language models, combined with the agents\u2019 ability to learn and evolve continuously, can lead to more informed and high-quality decisions. Foundation agents can consider a broader range of factors, insights, and perspectives, resulting in more comprehensive and well-rounded decision-making processes. Scalability and Adaptability: The modular and extensible architecture of foundation agents allows for seamless scaling and adaptation to changing requirements and environments. As new language models and technologies emerge, they can be easily integrated into the foundation agent ecosystem, ensuring continuous evolution and relevance. Cross-Domain Applicability: Foundation agents\u2019 versatility and ability to generalize make them applicable across a wide range of domains, including healthcare, finance, manufacturing, logistics, and more. By leveraging the collective knowledge and capabilities of multiple language models, foundation agents can tackle complex decision-making challenges in diverse industries and sectors. Enhanced Collaboration and Knowledge Sharing: The integration of multiple language models within the foundation agent ecosystem fosters enhanced collaboration and knowledge sharing. Different models can contribute their respective strengths", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 1950, "end": 2149}, "timestamp": "2025-04-20T23:24:04.543248", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_1950", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "the collective knowledge and capabilities of multiple language models, foundation agents can tackle complex decision-making challenges in diverse industries and sectors. Enhanced Collaboration and Knowledge Sharing: The integration of multiple language models within the foundation agent ecosystem fosters enhanced collaboration and knowledge sharing. Different models can contribute their respective strengths and expertise, enabling more comprehensive and synergistic decision-making processes. This cross-pollination of knowledge can drive innovation and lead to novel solutions for complex problems. Continuous Learning and Adaptation: Foundation agents are designed to learn and adapt continuously, ingesting new data and experiences to refine their decision-making capabilities. As environments and requirements evolve, foundation agents can seamlessly adapt, ensuring their relevance and effectiveness over time. This continuous learning aspect can drive sustained improvement and enable proactive response to emerging challenges and opportunities. Ethical and Responsible Decision-Making: By aligning the knowledge and values of foundation agents with existing LLMs and other language models, ethical principles and societal norms can be embedded into the decision-making process. Techniques like value alignment and ethical AI principles can be integrated, promoting responsible and trustworthy decision-making that considers the broader societal implications. Cost Optimization: The efficient resource utilization and load balancing capabilities of foundation agents can lead", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 2100, "end": 2299}, "timestamp": "2025-04-20T23:24:04.543252", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_2100", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "models, ethical principles and societal norms can be embedded into the decision-making process. Techniques like value alignment and ethical AI principles can be integrated, promoting responsible and trustworthy decision-making that considers the broader societal implications. Cost Optimization: The efficient resource utilization and load balancing capabilities of foundation agents can lead to significant cost savings in computational resources and infrastructure. By leveraging the collective power of multiple language models and optimizing resource allocation, foundation agents can reduce redundancy and maximize the utilization of available resources. Accelerated Innovation and Discovery: The synergistic combination of multiple language models within the foundation agent ecosystem can catalyze innovation and accelerate the pace of discovery. By combining diverse perspectives, insights, and approaches, foundation agents can uncover novel solutions, identify new opportunities, and drive breakthroughs in various domains. Democratization of Decision-Making: Foundation agents have the potential to democratize decision-making processes by making advanced AI capabilities more accessible and affordable. By leveraging the collective power of multiple language models, foundation agents can provide sophisticated decision-making support to a broader range of organizations and individuals, fostering inclusivity and equal opportunities. Challenges and Future Directions While the potential of foundation agents is promising, several challenges and future directions must be", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 2250, "end": 2449}, "timestamp": "2025-04-20T23:24:04.543257", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_2250", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "and affordable. By leveraging the collective power of multiple language models, foundation agents can provide sophisticated decision-making support to a broader range of organizations and individuals, fostering inclusivity and equal opportunities. Challenges and Future Directions While the potential of foundation agents is promising, several challenges and future directions must be considered to fully realize their transformative potential: Ethical and Bias Considerations: As foundation agents gain increasing influence in decision-making processes, it is crucial to address ethical concerns and mitigate potential biases. Techniques for ensuring fairness, accountability, and transparency in the decision-making processes of foundation agents must be developed and rigorously tested. Continuous monitoring and auditing mechanisms should be implemented to detect and mitigate biases that may arise from the underlying data or models used by the agents. Interpretability and Explainability: To foster trust and adoption, foundation agents must provide interpretable and explainable outputs, enabling stakeholders to understand the rationale behind their decisions. Techniques for interpreting the decision-making processes of foundation agents and generating human-understandable explanations should be explored and integrated into the system. Visualization tools and interfaces that facilitate the comprehension of the agents\u2019 decision-making processes can also enhance transparency and interpretability. Scalability and Performance Optimization: As the complexity and", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 2400, "end": 2599}, "timestamp": "2025-04-20T23:24:04.543261", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_2400", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "their decisions. Techniques for interpreting the decision-making processes of foundation agents and generating human-understandable explanations should be explored and integrated into the system. Visualization tools and interfaces that facilitate the comprehension of the agents\u2019 decision-making processes can also enhance transparency and interpretability. Scalability and Performance Optimization: As the complexity and scale of foundation agent systems grow, scalability and performance optimization challenges must be addressed. Techniques for efficient distributed computing, parallelization, and load balancing should be continuously refined and optimized to handle large-scale deployments. Hardware acceleration and optimization techniques, such as model quantization and pruning, should be explored to enhance computational efficiency and reduce resource requirements. Continuous Learning and Knowledge Evolution: Foundation agents must be capable of continuously learning and evolving to stay relevant and effective in dynamic environments. Techniques for seamless knowledge integration, transfer learning, and lifelong learning should be developed to enable the agents to adapt to new information and emerging trends. Robust mechanisms for verifying the accuracy and reliability of new knowledge acquired by the agents should be implemented to maintain the integrity of the decision-making processes. Human-AI Collaboration and Trust: Fostering effective collaboration between humans and foundation agents is crucial for their successful adoption and impact. User-friendly", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 2550, "end": 2749}, "timestamp": "2025-04-20T23:24:04.543265", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_2550", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "and emerging trends. Robust mechanisms for verifying the accuracy and reliability of new knowledge acquired by the agents should be implemented to maintain the integrity of the decision-making processes. Human-AI Collaboration and Trust: Fostering effective collaboration between humans and foundation agents is crucial for their successful adoption and impact. User-friendly interfaces and intuitive interaction mechanisms should be developed to facilitate seamless human-AI collaboration and enhance trust in the agents\u2019 recommendations. Techniques for incorporating human feedback, oversight, and adjustments into the decision-making processes of foundation agents should be explored to enhance their acceptance and reliability. Privacy and Security Considerations: As foundation agents handle and process potentially sensitive data, robust privacy and security measures must be implemented to protect individual and organizational data. Techniques such as differential privacy, secure multi-party computation, and homomorphic encryption should be explored to ensure data privacy and security. Stringent access controls, auditing mechanisms, and incident response protocols should be in place to prevent unauthorized access and mitigate potential security breaches. The Wrap Up Alright, let\u2019s bring this exploration of foundation agents to a close. After diving deep into the technical details and potential applications, it\u2019s clear that we\u2019re looking at a game-changing approach to decision-making in the", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 2700, "end": 2899}, "timestamp": "2025-04-20T23:24:04.543269", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_2700", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "should be in place to prevent unauthorized access and mitigate potential security breaches. The Wrap Up Alright, let\u2019s bring this exploration of foundation agents to a close. After diving deep into the technical details and potential applications, it\u2019s clear that we\u2019re looking at a game-changing approach to decision-making in the AI world. Think about it \u2014 we\u2019ve got these powerful language models that can tackle all sorts of tasks, but when it comes to making complex decisions in dynamic environments, they often struggle to put all the pieces together efficiently. That\u2019s where foundation agents come in, acting as the conductors that can orchestrate the collective intelligence of multiple models, big and small. It\u2019s like having a team of specialized experts, each with their own unique knowledge and skills, but instead of working in silos, they\u2019re collaborating seamlessly under the guidance of a highly adaptable coordinator \u2014 the foundation agent. This agent can quickly learn the ropes, distribute tasks to the right experts, integrate their insights, and make well-informed decisions that consider all angles. But it\u2019s not just about efficiency; foundation agents have the potential to drive innovation and push the boundaries of what\u2019s possible in decision-making. By fostering cross-pollination of", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 2850, "end": 3049}, "timestamp": "2025-04-20T23:24:04.543274", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_2850", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "agent can quickly learn the ropes, distribute tasks to the right experts, integrate their insights, and make well-informed decisions that consider all angles. But it\u2019s not just about efficiency; foundation agents have the potential to drive innovation and push the boundaries of what\u2019s possible in decision-making. By fostering cross-pollination of knowledge and enabling continuous learning, they can uncover novel solutions and open up new frontiers in various domains, from healthcare and finance to urban planning and environmental conservation. Now, let\u2019s be real \u2014 this paradigm shift won\u2019t happen overnight. There are hurdles to overcome, like ensuring ethical and unbiased decision-making, maintaining transparency and interpretability, scaling up performance, and addressing privacy and security concerns. But with the right mindset and a collaborative approach, these challenges can be tackled head-on. At the end of the day, foundation agents represent a future where advanced decision-making capabilities are democratized and accessible to a broader range of organizations and individuals. It\u2019s an exciting prospect that could revolutionize how we approach complex problems and drive positive change across industries. So, while the journey ahead may be filled with twists and turns, one thing\u2019s for sure \u2014 foundation agents are poised to be a game-changer, and it\u2019ll", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 3000, "end": 3199}, "timestamp": "2025-04-20T23:24:04.543278", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_3000", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "range of organizations and individuals. It\u2019s an exciting prospect that could revolutionize how we approach complex problems and drive positive change across industries. So, while the journey ahead may be filled with twists and turns, one thing\u2019s for sure \u2014 foundation agents are poised to be a game-changer, and it\u2019ll be thrilling to witness their impact unfold. Who\u2019s ready to embrace this paradigm shift and shape the future of decision-making? Count me in! AI Agents Llm Foundation Models Knowledge Graph 6 6 Looking to hide highlights? You can now hide them from the \u201c\u2022\u2022\u2022\u201d menu. Okay, got it Follow Written by Bijit Ghosh 3.4K Followers \u00b7 0 Following CTO | Senior Engineering Leader focused on Cloud Native | AI/ML | DevSecOps Follow No responses yet Soma Korada What are your thoughts? \ufeff Cancel Respond More from Bijit Ghosh Bijit Ghosh Top NVIDIA GPUs for LLM Inference Selecting the Optimal NVIDIA Hardware for LLM Inference \u2014 Your Guide to GPU Selection Sep 28, 2024 80 1 Bijit Ghosh Agentic Design Patterns From reflection to collaboration, Agentic Design Patterns are rewriting the rules of AI architecture \u2014 delivering smarter workflows\u2026 Jan 8 25 2 Bijit Ghosh When to Apply RAG vs Fine-Tuning", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 3150, "end": 3349}, "timestamp": "2025-04-20T23:24:04.543283", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_3150", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "Hardware for LLM Inference \u2014 Your Guide to GPU Selection Sep 28, 2024 80 1 Bijit Ghosh Agentic Design Patterns From reflection to collaboration, Agentic Design Patterns are rewriting the rules of AI architecture \u2014 delivering smarter workflows\u2026 Jan 8 25 2 Bijit Ghosh When to Apply RAG vs Fine-Tuning Leveraging the full potential of LLMs requires choosing the right technique between retrieval-augmented generation (RAG) and fine-tuning. Feb 26, 2024 480 Bijit Ghosh RAG Vs VectorDB Introduction to RAG and VectorDB Jan 28, 2024 428 5 See all from Bijit Ghosh Recommended from Medium In Data Science Collective by Ida Silfverski\u00f6ld Agentic AI: Comparing New Open-Source Frameworks By looking at functionality and learning curves 5d ago 601 18 Nayan Paul Building a Multi Agent Marketing Analytics Agentic Solution This blog has below 3 sections : Mar 31 22 Vipra Singh AI Agents: Multi-Agent Architectures ( Part-7) Discover AI agents, their design, and real-world applications. 6d ago 247 2 In Everyday AI by Manpreet Singh Goodbye RAG? Gemini 2.0 Flash Have Just Killed It! Alright!!! Feb 10 3.5K 151 In AI Advances by Debmalya Biswas AI Agents Marketplace & Discovery for Multi-agent Systems Why LLMs as a run-time execution engine for", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 3300, "end": 3499}, "timestamp": "2025-04-20T23:24:04.543287", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_3300", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "design, and real-world applications. 6d ago 247 2 In Everyday AI by Manpreet Singh Goodbye RAG? Gemini 2.0 Flash Have Just Killed It! Alright!!! Feb 10 3.5K 151 In AI Advances by Debmalya Biswas AI Agents Marketplace & Discovery for Multi-agent Systems Why LLMs as a run-time execution engine for Agentic AI systems do not Scale? Dec 27, 2024 673 11 In TDS Archive by Lu\u00eds Roque Agentic AI: Building Autonomous Systems from Scratch A Step-by-Step Guide to Creating Multi-Agent Frameworks in the Age of Generative AI Dec 13, 2024 860 20 See more recommendations Help Status About Careers Press Blog Privacy Rules Terms Text to speech", "url": "https://medium.com/@bijit211987/the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be", "position": {"start": 3450, "end": 3556}, "timestamp": "2025-04-20T23:24:04.543290", "chunk_id": "https_medium_com_@bijit211987_the-paradigm-shift-in-decision-making-with-foundation-agents-f27457b4c2be_3450", "title": "The Paradigm Shift in Decision Making with Foundation Agents | by Bijit Ghosh | Medium"}, {"text": "Introduction - Model Context Protocol Model Context Protocol home page Search... \u2318K Python SDK TypeScript SDK Java SDK Kotlin SDK C# SDK Get Started Introduction Quickstart Example Servers Example Clients FAQs Tutorials Building MCP with LLMs Debugging Inspector Concepts Core architecture Resources Prompts Tools Sampling Roots Transports Development What's New Roadmap Contributing Model Context Protocol home page Search... \u2318K GitHub GitHub Search... Navigation Get Started Introduction User Guide SDKs Specification User Guide SDKs Specification GitHub Get Started Introduction Copy page Get started with the Model Context Protocol (MCP) C# SDK released! Check out what else is new. MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools. \u200b Why MCP? MCP helps you build agents and complex workflows on top of LLMs. LLMs frequently need to integrate with data and tools, and MCP provides: A growing list of pre-built integrations that your LLM can directly plug into The flexibility to switch between LLM providers and vendors", "url": "https://modelcontextprotocol.io/introduction", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-20T23:25:04.024343", "chunk_id": "https_modelcontextprotocol_io_introduction_0", "title": "Introduction - Model Context Protocol"}, {"text": "tools. \u200b Why MCP? MCP helps you build agents and complex workflows on top of LLMs. LLMs frequently need to integrate with data and tools, and MCP provides: A growing list of pre-built integrations that your LLM can directly plug into The flexibility to switch between LLM providers and vendors Best practices for securing your data within your infrastructure \u200b General architecture At its core, MCP follows a client-server architecture where a host application can connect to multiple servers: Internet Your Computer MCP Protocol MCP Protocol MCP Protocol Web APIs Host with MCP Client (Claude, IDEs, Tools) MCP Server A MCP Server B MCP Server C Local Data Source A Local Data Source B Remote Service C MCP Hosts : Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP MCP Clients : Protocol clients that maintain 1:1 connections with servers MCP Servers : Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol Local Data Sources : Your computer\u2019s files, databases, and services that MCP servers can securely access Remote Services : External systems available over the internet (e.g., through APIs) that MCP servers can connect to \u200b Get started Choose", "url": "https://modelcontextprotocol.io/introduction", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-20T23:25:04.024354", "chunk_id": "https_modelcontextprotocol_io_introduction_150", "title": "Introduction - Model Context Protocol"}, {"text": "programs that each expose specific capabilities through the standardized Model Context Protocol Local Data Sources : Your computer\u2019s files, databases, and services that MCP servers can securely access Remote Services : External systems available over the internet (e.g., through APIs) that MCP servers can connect to \u200b Get started Choose the path that best fits your needs: \u200b Quick Starts For Server Developers Get started building your own server to use in Claude for Desktop and other clients For Client Developers Get started building your own client that can integrate with all MCP servers For Claude Desktop Users Get started using pre-built servers in Claude for Desktop \u200b Examples Example Servers Check out our gallery of official MCP servers and implementations Example Clients View the list of clients that support MCP integrations \u200b Tutorials Building MCP with LLMs Learn how to use LLMs like Claude to speed up your MCP development Debugging Guide Learn how to effectively debug MCP servers and integrations MCP Inspector Test and inspect your MCP servers with our interactive debugging tool MCP Workshop (Video, 2hr) \u200b Explore MCP Dive deeper into MCP\u2019s core concepts and capabilities: Core architecture Understand how MCP connects clients, servers, and LLMs", "url": "https://modelcontextprotocol.io/introduction", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-20T23:25:04.024359", "chunk_id": "https_modelcontextprotocol_io_introduction_300", "title": "Introduction - Model Context Protocol"}, {"text": "development Debugging Guide Learn how to effectively debug MCP servers and integrations MCP Inspector Test and inspect your MCP servers with our interactive debugging tool MCP Workshop (Video, 2hr) \u200b Explore MCP Dive deeper into MCP\u2019s core concepts and capabilities: Core architecture Understand how MCP connects clients, servers, and LLMs Resources Expose data and content from your servers to LLMs Prompts Create reusable prompt templates and workflows Tools Enable LLMs to perform actions through your server Sampling Let your servers request completions from LLMs Transports Learn about MCP\u2019s communication mechanism \u200b Contributing Want to contribute? Check out our Contributing Guide to learn how you can help improve MCP. \u200b Support and Feedback Here\u2019s how to get help or provide feedback: For bug reports and feature requests related to the MCP specification, SDKs, or documentation (open source), please create a GitHub issue For discussions or Q&A about the MCP specification, use the specification discussions For discussions or Q&A about other MCP open source components, use the organization discussions For bug reports, feature requests, and questions related to Claude.app and claude.ai\u2019s MCP integration, please see Anthropic\u2019s guide on How to Get Support Was this page helpful? Yes No For Server Developers github", "url": "https://modelcontextprotocol.io/introduction", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-20T23:25:04.024364", "chunk_id": "https_modelcontextprotocol_io_introduction_450", "title": "Introduction - Model Context Protocol"}, {"text": "the specification discussions For discussions or Q&A about other MCP open source components, use the organization discussions For bug reports, feature requests, and questions related to Claude.app and claude.ai\u2019s MCP integration, please see Anthropic\u2019s guide on How to Get Support Was this page helpful? Yes No For Server Developers github On this page Why MCP? General architecture Get started Quick Starts Examples Tutorials Explore MCP Contributing Support and Feedback", "url": "https://modelcontextprotocol.io/introduction", "position": {"start": 600, "end": 668}, "timestamp": "2025-04-20T23:25:04.024367", "chunk_id": "https_modelcontextprotocol_io_introduction_600", "title": "Introduction - Model Context Protocol"}, {"text": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog Join us on 4/24 for a demo-driven webinar on the industry's fastest guardrails and integrations with AWS SageMaker AI and NVIDIA. Register now Product Fiddler AI Observability Why Fiddler AI Observability Overview of key capabilities and benefits LLM Observability AI Observability for end-to-end LLMOps Fiddler Trust Service Guardrails and LLM application monitoring with Fiddler Trust Models ML Observability Deliver high performing AI solutions at scale Model Monitoring Detect model drift, assess performance and integrity, and set alerts NLP and CV Monitoring Monitor and uncover anomalies in unstructured models Explainable AI Understand the \u2018why\u2019 and \u2018how\u2019 behind your models Analytics Connect predictions with context to business alignment and value Responsible AI Mitigate bias and build a responsible AI culture See Fiddler in action Ready to get started? Request demo Solutions Use Cases Government Safeguard citizens and national security AI Governance, Risk Management, and Compliance (GRC) Enhance AI governance, mitigate risks, and meet compliance standards Customer Experience Deliver seamless customer experiences Lifetime Value Extend the customer lifetime value Lending and Trading Make fair and transparent lending decisions Partners Amazon SageMaker AI Unified MLOps for scalable model lifecycle management Google Cloud", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-20T23:40:12.693494", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_0", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "Governance, Risk Management, and Compliance (GRC) Enhance AI governance, mitigate risks, and meet compliance standards Customer Experience Deliver seamless customer experiences Lifetime Value Extend the customer lifetime value Lending and Trading Make fair and transparent lending decisions Partners Amazon SageMaker AI Unified MLOps for scalable model lifecycle management Google Cloud Deploy safe and trustworthy AI applications on Vertex AI NVIDIA NIM and NeMo Guardrails Monitor and protect LLM applications Databricks Accelerate production ML with a streamlined MLOps experience Datadog Gain complete visibility into the performance of your AI applications Become a partner Case Studies U.S. Navy decreased 97% time needed to update the ATR models Integral Ad Science scales transparent and compliant AI products with AI Observability Tide drives innovation, scale, and savings with AI Observability See customers Pricing Pricing Plans Choose the plan that\u2019s right for you Plan Comparison Compare platform capabilities and support across plans Platform Pricing Methodology Discover our simple and transparent pricing FAQs Pricing answers from frequently asked questions Build vs Buy Key considerations for buying AI Observability solution Contact Sales Have questions about pricing, plans, or Fiddler? Resources Learn Resource Library Discover reports, videos, and research Docs Get in-depth user guides and technical documentation Blog", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-20T23:40:12.693506", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_150", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "Discover our simple and transparent pricing FAQs Pricing answers from frequently asked questions Build vs Buy Key considerations for buying AI Observability solution Contact Sales Have questions about pricing, plans, or Fiddler? Resources Learn Resource Library Discover reports, videos, and research Docs Get in-depth user guides and technical documentation Blog Read product updates, data science research, and company news AI Forward Summit Watch recordings on how to operationalize production LLMs, and maximize the value of AI Connect Events Find out about upcoming events Webinars Learn from industry experts on pressing issues in MLOps and LLMOps Contact Us Get in touch with the Fiddler team Support Need help with the platform? Contact our support team The Ultimate Guide to LLM Monitoring Learn how enterprises should standardize and accelerate LLM application development, deployment, and management Read guide Company Company About Us Our mission and who we are Customers Learn how customers use Fiddler Careers We're hiring! Join fiddler to build trustworthy and responsible AI solutions Newsroom Explore recent news and press releases Security Enterprise-grade security and compliance standards Featured News Top 10 AI Companies Shaping the Tech World Bloomberg: AI-Equipped Underwater Drones Helping US Navy Scan for Threats AI Observability: The Key", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-20T23:40:12.693511", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_300", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "Fiddler Careers We're hiring! Join fiddler to build trustworthy and responsible AI solutions Newsroom Explore recent news and press releases Security Enterprise-grade security and compliance standards Featured News Top 10 AI Companies Shaping the Tech World Bloomberg: AI-Equipped Underwater Drones Helping US Navy Scan for Threats AI Observability: The Key to Unlocking the Full Potential of Large Language Models The insideBIGDATA IMPACT 50 List for Q3 2024 We're on a mission to build trust into AI Join us Contact us Request demo Try Free Guardrails Blog / Data Science / Integrated Gradients vs SHAP: How Should You Explain Your Predictions? Integrated Gradients vs SHAP: How Should You Explain Your Predictions? Published August 13, 2019 Last Edited April 15, 2025 Data Science Dan Frankowski , Fiddler AI Safeguard LLM Applications with the Fastest Guardrails in the Industry. Monitor your models, know the how and why behind decisions, and standardize LLMOps and MLOps best practices. Try Free Guardrails Some of the most accurate predictive models today are opaque models, meaning it is hard to really understand how they arrive at decisions. To enhance transparency, explainable AI techniques have emerged to analyze feature importance \u2014determining how much for a given prediction, each input", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-20T23:40:12.693516", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_450", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "MLOps best practices. Try Free Guardrails Some of the most accurate predictive models today are opaque models, meaning it is hard to really understand how they arrive at decisions. To enhance transparency, explainable AI techniques have emerged to analyze feature importance \u2014determining how much for a given prediction, each input feature contributes to a given prediction. Two well-known techniques are SHapley Additive exPlanations (SHAP) and Integrated Gradients (IG) . In fact, they each represent a different type of explanation algorithm: a Shapley-value-based algorithm (SHAP) and a gradient-based algorithm (IG). When comparing integrated gradients vs SHAP, a key distinction arises in how they compute features attributions. This post describes that difference. First, we need some background. Below, we review Shapley values , Shapley-value-based methods (including SHAP explainability), and gradient-based methods (including IG). Finally, we get back to our central question: When should you use a Shapley-value-based algorithm (like SHAP) versus a gradient-based explanation explanation algorithm (like IG)? What are Shapley values? The Shapley value (proposed by Lloyd Shapley in 1953) is a classic method to distribute the total gains of a collaborative game to a coalition of cooperating players. It is probably the only distribution with certain desirable properties. In the", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-20T23:40:12.693523", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_600", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "gradient-based explanation explanation algorithm (like IG)? What are Shapley values? The Shapley value (proposed by Lloyd Shapley in 1953) is a classic method to distribute the total gains of a collaborative game to a coalition of cooperating players. It is probably the only distribution with certain desirable properties. In the context of machine learning, SHAP values measure the contribution of each feature to a model\u2019s prediction. We can formulate this as a game for each prediction instance, where the \u201ctotal gains\u201d represent the prediction value, and the \u201cplayers\u201d are the model's features. The collaborative game is all of the model features cooperating to form a prediction value. The Shapley value efficiency property says the feature attributions should sum to the prediction value. The attributions can be negative or positive, since a feature can lower or raise a predicted value. There is a variant called the Aumann-Shapley value , extending the definition of the Shapley value to a game with many (or infinitely many) players, where each player plays only a minor role, if the worth function (the gains from including a coalition of players) is differentiable. What is a Shapley-value-based explanation method? A Shapley-value-based explanation method tries to approximate Shapley", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-20T23:40:12.693528", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_750", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "definition of the Shapley value to a game with many (or infinitely many) players, where each player plays only a minor role, if the worth function (the gains from including a coalition of players) is differentiable. What is a Shapley-value-based explanation method? A Shapley-value-based explanation method tries to approximate Shapley values of a given prediction by examining the effect of removing a feature under all possible combinations of presence or absence of the other features. In other words, this method looks at function values over subsets of features like F(x1, <absent> , x3, x4, \u2026, <absent>, \u2026 , xn ). How to evaluate a function F with one or more absent features is subtle. For example, SHAP (SHapely Additive exPlanations) estimates the model's behavior on an input with certain features absent by averaging over samples from those features drawn from the training set. In other words, F(x1, <absent>, x3, \u2026, xn) is estimated by the expected prediction when the missing feature x2 is sampled from the dataset. Exactly how that sample is chosen is important (for example marginal versus conditional distribution versus cluster centers of background data), but I will skip the fine details here. Once we define the model", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-20T23:40:12.693532", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_900", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "xn) is estimated by the expected prediction when the missing feature x2 is sampled from the dataset. Exactly how that sample is chosen is important (for example marginal versus conditional distribution versus cluster centers of background data), but I will skip the fine details here. Once we define the model function (F) for all subsets of the features, we can apply the Shapley values algorithm to compute feature attributions. Each feature\u2019s Shapley value is the contribution of the feature for all possible subsets of the other features. The \u201ckernel SHAP\u201d method from the SHAP paper computes the Shapley values of all features simultaneously by defining a weighted least squares regression whose solution is the Shapley values for all the features. The high-level point is that all these methods rely on taking subsets of features. This makes the theoretical version exponential in runtime: for N features, there are 2N combinations of presence and absence. That is too expensive for most N, so these methods approximate. Even with approximations, kernel SHAP can be slow. Also, we don\u2019t know of any systematic study of how good the approximation is. There are versions of SHAP specialized to different model architectures for speed. For example,", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-20T23:40:12.693536", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_1050", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "presence and absence. That is too expensive for most N, so these methods approximate. Even with approximations, kernel SHAP can be slow. Also, we don\u2019t know of any systematic study of how good the approximation is. There are versions of SHAP specialized to different model architectures for speed. For example, Tree SHAP computes all the subsets by cleverly keeping track of what proportion of all possible subsets flow down into each of the leaves of the tree. However, if your model architecture does not have a specialized algorithm like this, you have to fall back on kernel SHAP, or another naive (unoptimized) Shapley-value-based method. A Shapley-value-based method is attractive as it only requires black box access to the model (i.e. computing outputs from inputs), and there is a version agnostic to the model architecture. For instance, it does not matter whether the model function is discrete or continuous. The downside is that exactly computing the subsets is exponential in the number of features. What is a gradient-based explanation method? A gradient-based explanation method tries to explain a given prediction by using the gradient of (i.e. change in) the output with respect to the input features. Some methods like Integrated Gradients", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-20T23:40:12.693541", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_1200", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "is that exactly computing the subsets is exponential in the number of features. What is a gradient-based explanation method? A gradient-based explanation method tries to explain a given prediction by using the gradient of (i.e. change in) the output with respect to the input features. Some methods like Integrated Gradients (IG), gradient SHAP, GradCAM, and SmoothGrad literally apply the gradient operator. Other methods like DeepLift and LRP apply \u201cdiscrete gradients.\u201d \u200d Figure 1 from the IG paper , showing three paths between a baseline ( r1 , r 2 ) and an input (s 1 , s 2 ). Path P 2 , used by Integrated Gradients, simultaneously moves all features from off to on. Path P 1 moves along the edges, turning features on in sequence. Other paths like P 1 along different edges correspond to different sequences. SHAP computes the expected attribution over all such edge paths like P 1 . Let me describe IG , which has the advantage that it tries to approximate Aumann-Shapley values, which are axiomatically justified. IG operates by considering a straight line path, in feature space, from the input at hand (e.g., an image from a training set) to a certain baseline", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-20T23:40:12.693545", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_1350", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "P 1 . Let me describe IG , which has the advantage that it tries to approximate Aumann-Shapley values, which are axiomatically justified. IG operates by considering a straight line path, in feature space, from the input at hand (e.g., an image from a training set) to a certain baseline input (e.g., a black image), and integrating the gradient of the prediction with respect to input features (e.g., image pixels) along this path. This paper explains the intuition of the IG algorithm as follows. As the input varies along the straight line path between the baseline and the input at hand, the prediction moves along a trajectory from uncertainty to certainty (the final prediction probability). At each point on this trajectory, one can use the gradient with respect to the input features to attribute the change in the prediction probability back to the input features. IG aggregates these gradients along the trajectory using a path integral. IG (roughly) requires the prediction to be a continuous and piecewise differentiable function of the input features. (More precisely, it requires the function is continuous everywhere and the partial derivative along each input dimension satisfies Lebesgue\u2019s integrability condition, i.e., the set of discontinuous points", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-20T23:40:12.693550", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_1500", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "the trajectory using a path integral. IG (roughly) requires the prediction to be a continuous and piecewise differentiable function of the input features. (More precisely, it requires the function is continuous everywhere and the partial derivative along each input dimension satisfies Lebesgue\u2019s integrability condition, i.e., the set of discontinuous points has measure zero.) \u200d Figure 2 from the IG paper , showing which pixels were most important to each image label. Note it is important to choose a good baseline for IG to make sensible feature attributions. For example, if a black image is chosen as baseline, IG won\u2019t attribute importance to a completely black pixel in an actual image. The baseline value should both have a near-zero prediction, and also faithfully represent a complete absence of signal. IG is attractive as it is broadly applicable to all differentiable models, easy to implement in most machine learning frameworks (e.g., TensorFlow, PyTorch, Caffe), and computationally scalable to massive deep networks like Inception and ResNet with millions of neurons. When should you use a Shapley-value-based versus a gradient-based explanation method? Finally, the payoff! Our advice: If the model function is piecewise differentiable and you have access to the model gradient, use IG.", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 1650, "end": 1849}, "timestamp": "2025-04-20T23:40:12.693555", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_1650", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "PyTorch, Caffe), and computationally scalable to massive deep networks like Inception and ResNet with millions of neurons. When should you use a Shapley-value-based versus a gradient-based explanation method? Finally, the payoff! Our advice: If the model function is piecewise differentiable and you have access to the model gradient, use IG. Otherwise, use a Shapley-value-based method. Any model trained using gradient descent is differentiable. For example: neural networks, logistic regression, support vector machines. You can use IG with these. The major class of non-differentiable models is trees: boosted trees, random forests. They encode discrete values at the leaves. These require a Shapley-value-based method, like Tree SHAP. The IG algorithm is faster than a naive Shapley-value-based method like kernel SHAP, as it only requires computing the gradients of the model output on a few different inputs (typically 50). In contrast, a Shapley-value-based method requires computing the model output on a large number of inputs sampled from the exponentially huge subspace of all possible combinations of feature values. Computing gradients of differentiable models is efficient and well supported in most machine learning frameworks. However, a differentiable model is a prerequisite for IG. By contrast, a Shapley-value-based method makes no such assumptions. Several types", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 1800, "end": 1999}, "timestamp": "2025-04-20T23:40:12.693559", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_1800", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "of inputs sampled from the exponentially huge subspace of all possible combinations of feature values. Computing gradients of differentiable models is efficient and well supported in most machine learning frameworks. However, a differentiable model is a prerequisite for IG. By contrast, a Shapley-value-based method makes no such assumptions. Several types of input features that look discrete (hence might require a Shapley-value-based method) actually can be mapped to differentiable model types (which let us use IG). Let us walk through one example: text sentiment. Suppose we wish to attribute the sentiment prediction to the words in some input text. At first, it seems that such models may be non-differentiable as the input is discrete (a collection of words). However, differentiable models like deep neural networks can handle words by first mapping them to a high-dimensional continuous space using word embeddings . The model\u2019s prediction is a differentiable function of these embeddings. This makes it amenable to IG. Specifically, we attribute the prediction score to the embedding vectors. Since attributions are additive, we sum the attributions (retaining the sign) along the fields of each embedding vector and map it to the specific input word that the embedding corresponds to. A crucial question", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 1950, "end": 2149}, "timestamp": "2025-04-20T23:40:12.693564", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_1950", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "This makes it amenable to IG. Specifically, we attribute the prediction score to the embedding vectors. Since attributions are additive, we sum the attributions (retaining the sign) along the fields of each embedding vector and map it to the specific input word that the embedding corresponds to. A crucial question for IG is: what is the baseline prediction? For this text example, one option is to use the embedding vector corresponding to empty text. Some models take fixed length inputs by padding short sentences with a special \u201cno word\u201d token. In such cases, we can take the baseline as the embedding of a sentence with just \u201cno word\u201d tokens. For more on IG, see the paper or this how-to . SHAP vs Integrated Gradients for Model Explainability In many cases (a differentiable model with a gradient), you can use integrated gradients (IG) to get a more certain and possibly faster explanation of feature importance for a prediction. However, a Shapley-value-based method is required for other (non-differentiable) model types. At Fiddler, we support both SHAP and IG. (Full disclosure: Ankur Taly, a co-author of IG, works at Fiddler , and is a co-author of this post.) Contact us to talk to", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 2100, "end": 2299}, "timestamp": "2025-04-20T23:40:12.693568", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_2100", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "explanation of feature importance for a prediction. However, a Shapley-value-based method is required for other (non-differentiable) model types. At Fiddler, we support both SHAP and IG. (Full disclosure: Ankur Taly, a co-author of IG, works at Fiddler , and is a co-author of this post.) Contact us to talk to a Fiddler expert! Subscribe to our newsletter Monthly curated AI content, Fiddler updates, and more. * Subscribe Ready to try AI Observability? Request demo Try Free Guardrails Product AI Observability LLM Observability ML Observability Model Monitoring NLP and CV Monitoring Explainable AI Analytics Pricing Request Demo Explore Responsible AI Build vs Buy Governance, Risk Management, and Compliance Government Customer Experience Lending and Trading Lifetime Value Company About Us Careers Hiring Customers Partners Newsroom Security Contact Us Resources Blog Docs Resource Library Webinars Events Support Center Learn Fiddler Guardrails Now Native to NVIDIA NeMo Guardrails \u2014 Industry\u2019s Fastest Guardrails Fiddler Trust Service for LLM Scoring Four Ways that Enterprises Deploy LLMs AI Forward Summit \u00a9 2025 Fiddler AI. All rights reserved. Privacy Policy Terms of Use", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 2250, "end": 2424}, "timestamp": "2025-04-20T23:40:12.693576", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_2250", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "Service for LLM Scoring Four Ways that Enterprises Deploy LLMs AI Forward Summit \u00a9 2025 Fiddler AI. All rights reserved. Privacy Policy Terms of Use", "url": "https://www.fiddler.ai/blog/should-you-explain-your-predictions-with-shap-or-ig", "position": {"start": 2400, "end": 2424}, "timestamp": "2025-04-20T23:40:12.693580", "chunk_id": "https_www_fiddler_ai_blog_should-you-explain-your-predictions-with-shap-or-ig_2400", "title": "Integrated Gradients vs SHAP | Fiddler AI Blog | Fiddler AI Blog"}, {"text": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG? Pratik\u2019s Pakodas \ud83c\udf7f Subscribe Sign in Share this post Pratik\u2019s Pakodas \ud83c\udf7f LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG? Copy link Facebook Email Notes More LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG? A systematic approach to selection of encoders Pratik Bhavsar Aug 22, 2023 5 Share this post Pratik\u2019s Pakodas \ud83c\udf7f LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG? Copy link Facebook Email Notes More Share Come join Maxpool - A community to discuss industrial problems of leveraging LLM and vector search. You can join with this link - join.maxpool.ai . The industry is crazed about Retrieval Augmented Generation (RAG). Every company wants to build a Q&A engine which can help improve the efficiency of internal or external users. But often they are powered by semantic search where the selection of text encoder plays a crucial role in the performance. However, with the wide variety of options available, selecting the best text encoder can be a daunting task. Here we will explore these factors while choosing a text encoder: Leveraging MTEB benchmarks Custom evaluation", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-20T23:40:52.540425", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_0", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "by semantic search where the selection of text encoder plays a crucial role in the performance. However, with the wide variety of options available, selecting the best text encoder can be a daunting task. Here we will explore these factors while choosing a text encoder: Leveraging MTEB benchmarks Custom evaluation Tradeoff between private and public encoder Chunking Language support RAG evaluation LLMs for ranking Source: 101 ways to solve neural search Performance metrics Public benchmarks The most reliable source for assessing encoder capabilities is the MTEB (Massive Text Embedding Benchmark). With this benchmark, you can choose encoders according to vector dimension, average performance for retrieval, and model size. From the table we can see some open-source models beating OpenAI embedding API. Although we should take it with a pinch of salt because there is no perfect evaluation benchmark nor do we have a complete idea of what the models are trained on. People often ask - How good is the Cohere embedding API? Sadly no metrics are released for the same and it\u2019s difficult to confirm without testing it on your dataset. This holds true for all open-source encoders as well. Always evaluate on your dataset before the final selection.", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-20T23:40:52.540439", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_150", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "trained on. People often ask - How good is the Cohere embedding API? Sadly no metrics are released for the same and it\u2019s difficult to confirm without testing it on your dataset. This holds true for all open-source encoders as well. Always evaluate on your dataset before the final selection. This brings us to the topic of custom evaluation. https://huggingface.co/spaces/mteb/leaderboard Custom evaluation At times, encoders may not exhibit optimal performance when handling sensitive information. It may be necessary to assess their performance using our internal dataset. Method 1: Evaluation by annotation In this we generate a dataset and setup annotation to get a gold labels. Repeat the below process with different encoders & BM25 (to avoid biasing results for a particular encoder) Index your docs with an encoder Create synthetic queries using a generator like GPT with few shot examples Query index with these queries Save top 20 results for the query After getting the above results: Merge results for each query from all indexes Setup annotation guidelines (0/1 class, 1-10 match score, relative rank) Setup annotation task for rating the query<>result After the annotation you can use these metrics to quantify the performance. Read this blog to more about", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-20T23:40:52.540446", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_300", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "the query After getting the above results: Merge results for each query from all indexes Setup annotation guidelines (0/1 class, 1-10 match score, relative rank) Setup annotation task for rating the query<>result After the annotation you can use these metrics to quantify the performance. Read this blog to more about them. Rank unaware metrics Recall@k Precision@k F1@k Rank aware metrics MRR(Mean reciprocal rank) NDCG(Normalized Discounted Cumulative Gain) The annotation should be done in a way which allows us to know the desired metric. In my experience its easy to start with rank unaware metrics. Method 2: Evaluation by model In this we execute the data generation steps similar to Approach 1, but employ a language model (LLM) or a cross encoder as the evaluator. This enables the establishment of a relative ranking among all encoders. Subsequently, a manual assessment of the top three encoders can yield precise performance metrics. Method 3: Evaluation by clustering Conduct diverse clustering techniques and examine the coverage (quantity of data clustered) at distinct Silhouette scores (indicating vector similarity within clusters). Experiment with algorithms like DBSCAN and HDBSCAN, adjusting its parameters for optimal performance selection. Tradeoff between private and public encoder Query cost Semantic search requires", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-20T23:40:52.540451", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_450", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "3: Evaluation by clustering Conduct diverse clustering techniques and examine the coverage (quantity of data clustered) at distinct Silhouette scores (indicating vector similarity within clusters). Experiment with algorithms like DBSCAN and HDBSCAN, adjusting its parameters for optimal performance selection. Tradeoff between private and public encoder Query cost Semantic search requires the embedding API service to be highly available otherwise the latency of results can be poor which can affect the user experience adversely. APIs from embedding providers like OpenAI are highly available and we do not need to manage the hosting. In case we go with an open source model, we need to engineer the solution. This needs to be done based on the size of the model and latency requirements. Models upto 110M parameters(500MB) can be hosted with CPU instance but larger models may require GPU serving to cater to latency. Indexing cost Setting up the semantic search requires indexing of the docs which can cost a non-trivial amount. Since the indexing and querying require the same encoder, indexing cost will depend on the encoder service. It's prudent to store embeddings in a separate database to enable service resets or reindexing onto an alternate vector database. Failing to do", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-20T23:40:52.540456", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_600", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "indexing of the docs which can cost a non-trivial amount. Since the indexing and querying require the same encoder, indexing cost will depend on the encoder service. It's prudent to store embeddings in a separate database to enable service resets or reindexing onto an alternate vector database. Failing to do so would entail recalculating identical embeddings. Privacy Sensitive domains like finance and healthcare demand stringent data privacy. Consequently, services like OpenAI might not be viable options. Search latency Latency grows linearly with the dimension of the embeddings. So lesser dimension embeddings are preferred. Storage cost It's crucial to consider Vector DB storage cost if your application requires indexing millions of vectors\u2014storage cost scales linearly with dimension. Dimensions can be one of 384, 768, 1024, 1526. OpenAI embeddings come in 1526 dim which pushes storage costs to the maximum. For estimating storage cost, you can get a ballpark on avg units(phrase/sent) per doc and extrapolate it. Lets compare price for storing 1M vector in different vendors (updated on 13 Aug) Pinecone Weaviate Qdrant We can see price to be turning out in a similar range. Although there are other features like latency, hybrid search, metadata support, querying capabilities, pre-filtering, ease of", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-20T23:40:52.540461", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_750", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "per doc and extrapolate it. Lets compare price for storing 1M vector in different vendors (updated on 13 Aug) Pinecone Weaviate Qdrant We can see price to be turning out in a similar range. Although there are other features like latency, hybrid search, metadata support, querying capabilities, pre-filtering, ease of scalability and support SLA to be considered while selecting the vendor. Source: 101 ways to solve neural search Open source database You can also setup your own database with open source solutions like Milvus , Weaviate , Qdrant and Elasticsearch . This will incur an additional engineering overhead for maintenance and support. Subscribe Chunking How you decide to tokenise(break) the long text can decide the quality of your embeddings and hence the performance of you search system. You can read this post on how to deal with long documents. Alternatively, you can also exploit summarisation techniques to reduce noise, text size, encoding cost and storage cost with many summarisation techniques with LLMs. Chunking is an important yet underrated topic. It can require domain expertise similar to how we do feature engineering. For ex. chunking for python codebases might be done with prefixes like def/class. Language support When the application is", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-20T23:40:52.540465", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_900", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "size, encoding cost and storage cost with many summarisation techniques with LLMs. Chunking is an important yet underrated topic. It can require domain expertise similar to how we do feature engineering. For ex. chunking for python codebases might be done with prefixes like def/class. Language support When the application is across the geography, the language of user can be different from the document. Approach 1 Use a multilingual sentence encoder to represent text from many language to similar vectors. Cohere provides a multilingual embedding API. Since we do not have the metrics for it in comparison to others, do evaluate before using. You will find some open-source options on MTEB. Approach 2 Use an English encoder alongside a translation system Detect the language of user query (ex. French) Translate query to English (French \u2192 English) Embed query Search documents Translate all top-scoring documents to the user\u2019s language (English \u2192 French) LLMs for ranking There is also a motion on using LLM for ranking. They seem to be beating powerful rerankers but are also prohibitively expensive and slow. You can read these for more information on the approaches: Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting Is ChatGPT", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-20T23:40:52.540471", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_1050", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "LLMs for ranking There is also a motion on using LLM for ranking. They seem to be beating powerful rerankers but are also prohibitively expensive and slow. You can read these for more information on the approaches: Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent Using LLM\u2019s for Retrieval and Reranking by Llamaindex Large Language Models and Search by Weaviate RAG evaluation Often companies use encoders for semantic search to power RAG system. The performance of these systems can be improved with better combination of various parameters. This is a lightweight evaluation tool for question-answering using Langchain. It is not exhaustive but can give you a good starting point. This is how it works: Ask the user to input a set of documents of interest Apply an LLM ( GPT-3.5-turbo ) to auto-generate question - answer pairs from these docs Generate a question-answering chain with a specified set of UI-chosen configurations Use the chain to generate a response to each question Use an LLM ( GPT-3.5-turbo ) to score the response relative to the answer Explore scoring across various chain configurations https://github.com/rlancemartin/auto-evaluator Conclusion In conclusion,", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-20T23:40:52.540476", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_1200", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "answer pairs from these docs Generate a question-answering chain with a specified set of UI-chosen configurations Use the chain to generate a response to each question Use an LLM ( GPT-3.5-turbo ) to score the response relative to the answer Explore scoring across various chain configurations https://github.com/rlancemartin/auto-evaluator Conclusion In conclusion, the process of selecting the suitable text encoder for semantic search is a critical step that significantly impacts the performance of your application. With the vast array of options available, making this choice might seem overwhelming, but a systematic approach can simplify making the decision. Subscribe Come join Maxpool - A community to discuss industrial problems of leveraging LLM and vector search! Connect with me on Medium , Twitter & LinkedIn . 5 Likes 5 Share this post Pratik\u2019s Pakodas \ud83c\udf7f LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG? Copy link Facebook Email Notes More Share Previous Next Discussion about this post Comments Restacks Top Latest Discussions How To Become A Full Stack Data Scientist? Years of sweating and burning fingers Mar 26, 2021 \u2022 Pratik Bhavsar 17 Share this post Pratik\u2019s Pakodas \ud83c\udf7f How To Become A Full Stack Data Scientist? Copy link Facebook Email", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-20T23:40:52.540482", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_1350", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "Previous Next Discussion about this post Comments Restacks Top Latest Discussions How To Become A Full Stack Data Scientist? Years of sweating and burning fingers Mar 26, 2021 \u2022 Pratik Bhavsar 17 Share this post Pratik\u2019s Pakodas \ud83c\udf7f How To Become A Full Stack Data Scientist? Copy link Facebook Email Notes More 7 LLM Chronicles #1: Will LLMs Make NLP Scientists Jobless? \"In retrospect, I wish I had known more about the hazards and difficulties of working in AI.\" -- NLP scientist Mar 21, 2023 \u2022 Pratik Bhavsar 8 Share this post Pratik\u2019s Pakodas \ud83c\udf7f LLM Chronicles #1: Will LLMs Make NLP Scientists Jobless? Copy link Facebook Email Notes More LLM Chronicles #9: The Power Law of MCP Understanding the basics of MCP Mar 11 \u2022 Pratik Bhavsar 7 Share this post Pratik\u2019s Pakodas \ud83c\udf7f LLM Chronicles #9: The Power Law of MCP Copy link Facebook Email Notes More See all Ready for more? Subscribe \u00a9 2025 Pratik Privacy \u2219 Terms \u2219 Collection notice Start writing Get the app Substack is the home for great culture Share Copy link Facebook Email Notes More Create your profile Name (Required) Handle Bio Email (Required) Subscribe to the newsletter undefined subscriptions will be", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-20T23:40:52.540487", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_1500", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "all Ready for more? Subscribe \u00a9 2025 Pratik Privacy \u2219 Terms \u2219 Collection notice Start writing Get the app Substack is the home for great culture Share Copy link Facebook Email Notes More Create your profile Name (Required) Handle Bio Email (Required) Subscribe to the newsletter undefined subscriptions will be displayed on your profile ( edit ) Skip for now I agree to Substack's Terms of Use , and acknowledge its Information Collection Notice and Privacy Policy . Save & Post Comment Only paid subscribers can comment on this post Subscribe Already a paid subscriber? Sign in Check your email For your security, we need to re-authenticate you. Click the link we sent to , or click here to sign in . This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts", "url": "https://pakodas.substack.com/p/llm-chronicles-8-how-to-select-encoder", "position": {"start": 1650, "end": 1785}, "timestamp": "2025-04-20T23:40:52.540491", "chunk_id": "https_pakodas_substack_com_p_llm-chronicles-8-how-to-select-encoder_1650", "title": "LLM Chronicles #8: How To Select Encoder For Semantic Search & RAG?"}, {"text": "Azure Functions best practices | Microsoft Learn Skip to main content Skip to Ask Learn chat experience The future is yours Microsoft Build \u00b7 May 19 \u2013 22, 2025 Join developers and AI innovators to refactor your skills at Microsoft Build. Register now Dismiss alert This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Download Microsoft Edge More info about Internet Explorer and Microsoft Edge Learn Suggestions will filter as you type Sign in Profile Settings Sign out Learn Discover Documentation In-depth articles on Microsoft developer tools and technologies Training Personalized learning paths and courses Credentials Globally recognized, industry-endorsed credentials Q&A Technical questions and answers moderated by Microsoft Code Samples Code sample library for Microsoft developer tools and technologies Assessments Interactive, curated guidance and recommendations Shows Thousands of hours of original programming from Microsoft experts Microsoft AI Skills Fest Register Now! April 8 - May 28, 2025 Product documentation ASP.NET Azure Dynamics 365 Microsoft 365 Microsoft Copilot Microsoft Entra Microsoft Exchange Microsoft Fabric Microsoft Intune .NET Power Apps Microsoft Power BI Power Platform Microsoft Purview SharePoint SQL Microsoft Teams Visual Studio Windows Windows Server View all products", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-20T23:41:40.112459", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_0", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "Skills Fest Register Now! April 8 - May 28, 2025 Product documentation ASP.NET Azure Dynamics 365 Microsoft 365 Microsoft Copilot Microsoft Entra Microsoft Exchange Microsoft Fabric Microsoft Intune .NET Power Apps Microsoft Power BI Power Platform Microsoft Purview SharePoint SQL Microsoft Teams Visual Studio Windows Windows Server View all products Microsoft AI Skills Fest Register Now! April 8 - May 28, 2025 Development languages C++ C# DAX Java OData OpenAPI Power Query M VBA Microsoft AI Skills Fest Register Now! April 8 - May 28, 2025 Topics Learn for Organizations Artificial intelligence Compliance DevOps Platform engineering Security Microsoft AI Skills Fest Register Now! April 8 - May 28, 2025 Suggestions will filter as you type Sign in Profile Settings Sign out Azure Products Popular products Azure AI Services Azure App Service Azure Databricks Azure DevOps Azure Functions Azure Monitor Azure Virtual Machines Popular categories Compute Networking Storage AI & machine learning Analytics Databases Security View all products Architecture Cloud Adoption Framework Well-Architected Framework Azure Architecture Center Develop Python .NET JavaScript Java PowerShell Azure CLI View all developer resources Learn Azure Start your AI learning assessment Top learning paths Cloud concepts AI fundamentals Intro to generative AI Azure Architecture fundamentals Earn", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-20T23:41:40.112476", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_150", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "learning Analytics Databases Security View all products Architecture Cloud Adoption Framework Well-Architected Framework Azure Architecture Center Develop Python .NET JavaScript Java PowerShell Azure CLI View all developer resources Learn Azure Start your AI learning assessment Top learning paths Cloud concepts AI fundamentals Intro to generative AI Azure Architecture fundamentals Earn credentials Instructor-led courses View all training Troubleshooting Resources Product overview Latest blog posts Pricing information Support options More Products Popular products Azure AI Services Azure App Service Azure Databricks Azure DevOps Azure Functions Azure Monitor Azure Virtual Machines Popular categories Compute Networking Storage AI & machine learning Analytics Databases Security View all products Architecture Cloud Adoption Framework Well-Architected Framework Azure Architecture Center Develop Python .NET JavaScript Java PowerShell Azure CLI View all developer resources Learn Azure Start your AI learning assessment Top learning paths Cloud concepts AI fundamentals Intro to generative AI Azure Architecture fundamentals Earn credentials Instructor-led courses View all training Troubleshooting Resources Product overview Latest blog posts Pricing information Support options Portal Free account Search Suggestions will filter as you type Functions Documentation Overview Quickstarts Create your first function C# Java JavaScript PowerShell Python TypeScript Other (Go/Rust) Resource Manager Azure Container Apps Connect to storage Connect to a", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-20T23:41:40.112481", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_300", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "all training Troubleshooting Resources Product overview Latest blog posts Pricing information Support options Portal Free account Search Suggestions will filter as you type Functions Documentation Overview Quickstarts Create your first function C# Java JavaScript PowerShell Python TypeScript Other (Go/Rust) Resource Manager Azure Container Apps Connect to storage Connect to a database Connect to OpenAI Tutorials Samples Concepts Best practices General best practices Performance and reliability Manage connections Storage considerations Error handling and function retries Security Compare runtime versions Hosting and scale Deployment options Events and messaging Triggers and bindings Frameworks Security Reliability Diagnostics Consumption plan costs Serverless REST APIs Networking options IP addresses Custom handlers Languages How-to guides Migration guides Reference Resources Download PDF Table of contents Exit focus mode Learn Azure Functions Learn Azure Functions Ask Learn Ask Learn Read in English Save Add to Collections Add to plan Table of contents Read in English Add to Collections Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories . Access to this page requires authorization. You can try changing directories . Best practices for reliable Azure Functions Article 07/13/2022 28 contributors Feedback In", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-20T23:41:40.112485", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_450", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories . Access to this page requires authorization. You can try changing directories . Best practices for reliable Azure Functions Article 07/13/2022 28 contributors Feedback In this article Choose the correct hosting plan Configure storage correctly Organize your functions Optimize deployments Write robust functions Design for security Consider concurrency Maximize availability Monitor effectively Build in redundancy Next steps Show 7 more Azure Functions is an event-driven, compute-on-demand experience that extends the existing Azure App Service application platform with capabilities to implement code triggered by events occurring in Azure, in third-party service, and in on-premises systems. Functions lets you build solutions by connecting to data sources or messaging solutions, which makes it easier to process and react to events. Functions runs on Azure data centers, which are complex with many integrated components. In a hosted cloud environment, it's expected that VMs can occasionally restart or move, and systems upgrades will occur. Your functions apps also likely depend on external APIs, Azure Services, and other databases, which are also prone to periodic unreliability. This article details some best", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-20T23:41:40.112488", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_600", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "complex with many integrated components. In a hosted cloud environment, it's expected that VMs can occasionally restart or move, and systems upgrades will occur. Your functions apps also likely depend on external APIs, Azure Services, and other databases, which are also prone to periodic unreliability. This article details some best practices for designing and deploying efficient function apps that remain healthy and perform well in a cloud-based environment. Choose the correct hosting plan When you create a function app in Azure, you must choose a hosting plan for your app. The plan you choose has an effect on performance, reliability, and cost. These are the hosting plans provided by Azure Functions: Consumption plan Flex Consumption plan Premium plan Dedicated (App Service) plan In the context of the App Service platform, the Premium plan used to dynamically host your functions is the Elastic Premium plan (EP). There are other Dedicated (App Service) plans called Premium. To learn more, see the Premium plan article. The hosting plan you choose determines the following behaviors: How your function app is scaled based on demand and how instances allocation is managed. The resources available to each function app instance. Support for advanced functionality, such as", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-20T23:41:40.112492", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_750", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "Service) plans called Premium. To learn more, see the Premium plan article. The hosting plan you choose determines the following behaviors: How your function app is scaled based on demand and how instances allocation is managed. The resources available to each function app instance. Support for advanced functionality, such as Azure Virtual Network connectivity. To learn more about choosing the correct hosting plan and for a detailed comparison between the plans, see Azure Functions hosting options . It's important that you choose the correct plan when you create your function app. Functions provides a limited ability to switch your hosting plan, primarily between Consumption and Elastic Premium plans. To learn more, see Plan migration . Configure storage correctly Functions requires a storage account be associated with your function app. The storage account connection is used by the Functions host for operations such as managing triggers and logging function executions. It's also used when dynamically scaling function apps. To learn more, see Storage considerations for Azure Functions . A misconfigured file system or storage account in your function app can affect the performance and availability of your functions. For help with troubleshooting an incorrectly configured storage account, see the storage troubleshooting", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-20T23:41:40.112496", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_900", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "also used when dynamically scaling function apps. To learn more, see Storage considerations for Azure Functions . A misconfigured file system or storage account in your function app can affect the performance and availability of your functions. For help with troubleshooting an incorrectly configured storage account, see the storage troubleshooting article. Storage connection settings Function apps that scale dynamically can run either from an Azure Files endpoint in your storage account or from the file servers associated with your scaled-out instances. This behavior is controlled by the following application settings: WEBSITE_CONTENTAZUREFILECONNECTIONSTRING WEBSITE_CONTENTSHARE These settings are only supported when you are running in a Premium plan or in a Consumption plan on Windows. When you create your function app either in the Azure portal or by using Azure CLI or Azure PowerShell, these settings are created for your function app when needed. When you create your resources from an Azure Resource Manager template (ARM template), you need to also include WEBSITE_CONTENTAZUREFILECONNECTIONSTRING in the template. On your first deployment using an ARM template, don't include WEBSITE_CONTENTSHARE , which is generated for you. You can use the following ARM template examples to help correctly configure these settings: Consumption plan Dedicated plan Premium plan", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-20T23:41:40.112499", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_1050", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "Manager template (ARM template), you need to also include WEBSITE_CONTENTAZUREFILECONNECTIONSTRING in the template. On your first deployment using an ARM template, don't include WEBSITE_CONTENTSHARE , which is generated for you. You can use the following ARM template examples to help correctly configure these settings: Consumption plan Dedicated plan Premium plan with VNET integration Consumption plan with a deployment slot Important The Azure Files service doesn't currently support identity-based connections. For more information, see Create an app without Azure Files . Storage account configuration When creating a function app, you must create or link to a general-purpose Azure Storage account that supports Blob, Queue, and Table storage. Functions relies on Azure Storage for operations such as managing triggers and logging function executions. The storage account connection string for your function app is found in the AzureWebJobsStorage and WEBSITE_CONTENTAZUREFILECONNECTIONSTRING application settings. Keep in mind the following considerations when creating this storage account: To reduce latency, create the storage account in the same region as the function app. To improve performance in production, use a separate storage account for each function app. This is especially true with Durable Functions and Event Hubs triggered functions. For Event Hubs triggered functions, don't use an account", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-20T23:41:40.112503", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_1200", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "To reduce latency, create the storage account in the same region as the function app. To improve performance in production, use a separate storage account for each function app. This is especially true with Durable Functions and Event Hubs triggered functions. For Event Hubs triggered functions, don't use an account with Data Lake Storage enabled . Handling large data sets When running on Linux, you can add extra storage by mounting a file share. Mounting a share is a convenient way for a function to process a large existing data set. To learn more, see Mount file shares . Organize your functions As part of your solution, you likely develop and publish multiple functions. These functions are often combined into a single function app, but they can also run in separate function apps. In Premium and Dedicated (App Service) hosting plans, multiple function apps can also share the same resources by running in the same plan. How you group your functions and function apps can impact the performance, scaling, configuration, deployment, and security of your overall solution. For Consumption and Premium plan, all functions in a function app are dynamically scaled together. For more information on how to organize your", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-20T23:41:40.112507", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_1350", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "by running in the same plan. How you group your functions and function apps can impact the performance, scaling, configuration, deployment, and security of your overall solution. For Consumption and Premium plan, all functions in a function app are dynamically scaled together. For more information on how to organize your functions, see Function organization best practices . Optimize deployments When deploying a function app, it's important to keep in mind that the unit of deployment for functions in Azure is the function app. All functions in a function app are deployed at the same time, usually from the same deployment package. Consider these options for a successful deployment: Have your functions run from the deployment package. This run from package approach provides the following benefits: Reduces the risk of file copy locking issues. Can be deployed directly to a production app, which does trigger a restart. Know that all files in the package are available to your app. Improves the performance of ARM template deployments. May reduce cold-start times, particularly for JavaScript functions with large npm package trees. Consider using continuous deployment to connect deployments to your source control solution. Continuous deployments also let you run from the deployment package.", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-20T23:41:40.112510", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_1500", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "in the package are available to your app. Improves the performance of ARM template deployments. May reduce cold-start times, particularly for JavaScript functions with large npm package trees. Consider using continuous deployment to connect deployments to your source control solution. Continuous deployments also let you run from the deployment package. For Premium plan hosting , consider adding a warmup trigger to reduce latency when new instances are added. To learn more, see Azure Functions warm-up trigger . To minimize deployment downtime and to be able to roll back deployments, consider using deployment slots. To learn more, see Azure Functions deployment slots . Write robust functions There are several design principles you can follow when writing your function code that help with general performance and availability of your functions. These principles include: Avoid long running functions. Plan cross-function communication. Write functions to be stateless. Write defensive functions. Because transient failures are common in cloud computing, you should use a retry pattern when accessing cloud-based resources. Many triggers and bindings already implement retry. Design for security Security is best considered during the planning phase and not after your functions are ready to go. To Learn how to securely develop and deploy functions,", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 1650, "end": 1849}, "timestamp": "2025-04-20T23:41:40.112514", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_1650", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "common in cloud computing, you should use a retry pattern when accessing cloud-based resources. Many triggers and bindings already implement retry. Design for security Security is best considered during the planning phase and not after your functions are ready to go. To Learn how to securely develop and deploy functions, see Securing Azure Functions . Consider concurrency As demand builds on your function app as a result of incoming events, function apps running in Consumption and Premium plans are scaled out. It's important to understand how your function app responds to load and how the triggers can be configured to handle incoming events. For a general overview, see Event-driven scaling in Azure Functions . Dedicated (App Service) plans require you to provide for scaling out your function apps. Worker process count In some cases, it's more efficient to handle the load by creating multiple processes, called language worker processes, in the instance before scale-out. The maximum number of language worker processes allowed is controlled by the FUNCTIONS_WORKER_PROCESS_COUNT setting. The default for this setting is 1 , which means that multiple processes aren't used. After the maximum number of processes are reached, the function app is scaled out to more instances", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 1800, "end": 1999}, "timestamp": "2025-04-20T23:41:40.112519", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_1800", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "the instance before scale-out. The maximum number of language worker processes allowed is controlled by the FUNCTIONS_WORKER_PROCESS_COUNT setting. The default for this setting is 1 , which means that multiple processes aren't used. After the maximum number of processes are reached, the function app is scaled out to more instances to handle the load. This setting doesn't apply for C# class library functions , which run in the host process. When using FUNCTIONS_WORKER_PROCESS_COUNT on a Premium plan or Dedicated (App Service) plan, keep in mind the number of cores provided by your plan. For example, the Premium plan EP2 provides two cores, so you should start with a value of 2 and increase by two as needed, up to the maximum. Trigger configuration When planning for throughput and scaling, it's important to understand how the different types of triggers process events. Some triggers allow you to control the batching behaviors and manage concurrency. Often adjusting the values in these options can help each instance scale appropriately for the demands of the invoked functions. These configuration options are applied to all triggers in a function app, and are maintained in the host.json file for the app. See the Configuration section of", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 1950, "end": 2149}, "timestamp": "2025-04-20T23:41:40.112522", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_1950", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "and manage concurrency. Often adjusting the values in these options can help each instance scale appropriately for the demands of the invoked functions. These configuration options are applied to all triggers in a function app, and are maintained in the host.json file for the app. See the Configuration section of the specific trigger reference for settings details. To learn more about how Functions processes message streams, see Azure Functions reliable event processing . Plan for connections Function apps running in Consumption plan are subject to connection limits. These limits are enforced on a per-instance basis. Because of these limits and as a general best practice, you should optimize your outbound connections from your function code. To learn more, see Manage connections in Azure Functions . Language-specific considerations For your language of choice, keep in mind the following considerations: C# Java JavaScript PowerShell Python Use async code but avoid blocking calls . Use cancellation tokens (in-process only). For applications that are a mix of CPU-bound and IO-bound operations, consider using more worker processes . Use async and await . Use multiple worker processes for CPU bound applications . Review the concurrency considerations . Improve throughput performance of Python apps in Azure", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 2100, "end": 2299}, "timestamp": "2025-04-20T23:41:40.112526", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_2100", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": ". Use cancellation tokens (in-process only). For applications that are a mix of CPU-bound and IO-bound operations, consider using more worker processes . Use async and await . Use multiple worker processes for CPU bound applications . Review the concurrency considerations . Improve throughput performance of Python apps in Azure Functions Maximize availability Cold start is a key consideration for serverless architectures. To learn more, see Cold starts . If cold start is a concern for your scenario, you can find a deeper dive in the post Understanding serverless cold start . Premium plan is the recommended plan for reducing colds starts while maintaining dynamic scale. You can use the following guidance to reduce cold starts and improve availability in all three hosting plans. Expand table Plan Guidance Premium plan \u2022 Implement a Warmup trigger in your function app \u2022 Set the values for Always-Ready instances and Max Burst limit \u2022 Use virtual network trigger support when using non-HTTP triggers on a virtual network Dedicated plans \u2022 Run on at least two instances with Azure App Service Health Check enabled \u2022 Implement autoscaling Consumption plan \u2022 Review your use of Singleton patterns and the concurrency settings for bindings and triggers", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 2250, "end": 2449}, "timestamp": "2025-04-20T23:41:40.112531", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_2250", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "\u2022 Use virtual network trigger support when using non-HTTP triggers on a virtual network Dedicated plans \u2022 Run on at least two instances with Azure App Service Health Check enabled \u2022 Implement autoscaling Consumption plan \u2022 Review your use of Singleton patterns and the concurrency settings for bindings and triggers to avoid artificially placing limits on how your function app scales. \u2022 Review the functionAppScaleLimit setting, which can limit scale-out \u2022 Check for a Daily Usage Quota (GB-Sec) limit set during development and testing. Consider removing this limit in production environments. Monitor effectively Azure Functions offers built-in integration with Azure Application Insights to monitor your function execution and traces written from your code. To learn more, see Monitor Azure Functions . Azure Monitor also provides facilities for monitoring the health of the function app itself. To learn more, see Monitoring with Azure Monitor . You should be aware of the following considerations when using Application Insights integration to monitor your functions: Make sure that the AzureWebJobsDashboard application setting is removed. This setting was supported in older version of Functions. If it exists, removing AzureWebJobsDashboard improves performance of your functions. Review the Application Insights logs . If data you expect to", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 2400, "end": 2599}, "timestamp": "2025-04-20T23:41:40.112538", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_2400", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "following considerations when using Application Insights integration to monitor your functions: Make sure that the AzureWebJobsDashboard application setting is removed. This setting was supported in older version of Functions. If it exists, removing AzureWebJobsDashboard improves performance of your functions. Review the Application Insights logs . If data you expect to find is missing, consider adjusting the sampling settings to better capture your monitoring scenario. You can use the excludedTypes setting to exclude certain types from sampling, such as Request or Exception . To learn more, see Configure sampling . Azure Functions also allows you to send system-generated and user-generated logs to Azure Monitor Logs . Integration with Azure Monitor Logs is currently in preview. Build in redundancy Your business needs might require that your functions always be available, even during a data center outage. To learn how to use a multi-regional approach to keep your critical functions always running, see Azure Functions geo-disaster recovery and high-availability . Next steps Manage your function app Feedback Was this page helpful? Yes No Provide product feedback | Get help at Microsoft Q&A Additional resources Documentation Improve Azure Functions performance and reliability Learn how to best improve the performance and reliability of running your", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 2550, "end": 2749}, "timestamp": "2025-04-20T23:41:40.112542", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_2550", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "Azure Functions geo-disaster recovery and high-availability . Next steps Manage your function app Feedback Was this page helpful? Yes No Provide product feedback | Get help at Microsoft Q&A Additional resources Documentation Improve Azure Functions performance and reliability Learn how to best improve the performance and reliability of running your functions in Azure. Manage connections in Azure Functions Learn how to avoid performance problems in Azure Functions by using static connection clients. Azure Functions scale and hosting Compare the various options you need to consider when choosing a hosting plan in which to run your function app in Azure Functions. Event-driven scaling in Azure Functions Explains the scaling behaviors of Consumption plan and Premium plan function apps. Azure Functions warmup trigger Understand how to use the warmup trigger in Azure Functions. Guidance for developing Azure Functions Learn the Azure Functions concepts and techniques that you need to develop functions in Azure, across all programming languages and bindings. Configure function app settings in Azure Functions Learn how to configure function app settings in Azure Functions. host.json reference for Azure Functions 2.x Describes the various available settings for the Azure Functions host that can be set in the host.json file for the", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 2700, "end": 2899}, "timestamp": "2025-04-20T23:41:40.112546", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_2700", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "Azure, across all programming languages and bindings. Configure function app settings in Azure Functions Learn how to configure function app settings in Azure Functions. host.json reference for Azure Functions 2.x Describes the various available settings for the Azure Functions host that can be set in the host.json file for the Functions v4 runtime. Show 5 more Training Learning path Use advance techniques in canvas apps to perform custom updates and optimization - Training Use advance techniques in canvas apps to perform custom updates and optimization Certification Microsoft Certified: Azure Developer Associate - Certifications Build end-to-end solutions in Microsoft Azure to create Azure Functions, implement and manage web apps, develop solutions utilizing Azure storage, and more. Additional resources Training Learning path Use advance techniques in canvas apps to perform custom updates and optimization - Training Use advance techniques in canvas apps to perform custom updates and optimization Certification Microsoft Certified: Azure Developer Associate - Certifications Build end-to-end solutions in Microsoft Azure to create Azure Functions, implement and manage web apps, develop solutions utilizing Azure storage, and more. Documentation Improve Azure Functions performance and reliability Learn how to best improve the performance and reliability of running your functions in Azure. Manage connections", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 2850, "end": 3049}, "timestamp": "2025-04-20T23:41:40.112549", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_2850", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "Developer Associate - Certifications Build end-to-end solutions in Microsoft Azure to create Azure Functions, implement and manage web apps, develop solutions utilizing Azure storage, and more. Documentation Improve Azure Functions performance and reliability Learn how to best improve the performance and reliability of running your functions in Azure. Manage connections in Azure Functions Learn how to avoid performance problems in Azure Functions by using static connection clients. Azure Functions scale and hosting Compare the various options you need to consider when choosing a hosting plan in which to run your function app in Azure Functions. Event-driven scaling in Azure Functions Explains the scaling behaviors of Consumption plan and Premium plan function apps. Azure Functions warmup trigger Understand how to use the warmup trigger in Azure Functions. Guidance for developing Azure Functions Learn the Azure Functions concepts and techniques that you need to develop functions in Azure, across all programming languages and bindings. Configure function app settings in Azure Functions Learn how to configure function app settings in Azure Functions. host.json reference for Azure Functions 2.x Describes the various available settings for the Azure Functions host that can be set in the host.json file for the Functions v4 runtime. Show 5", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 3000, "end": 3199}, "timestamp": "2025-04-20T23:41:40.112552", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_3000", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "and bindings. Configure function app settings in Azure Functions Learn how to configure function app settings in Azure Functions. host.json reference for Azure Functions 2.x Describes the various available settings for the Azure Functions host that can be set in the host.json file for the Functions v4 runtime. Show 5 more In this article English (United States) Your Privacy Choices Theme Light Dark High contrast Previous Versions Blog Contribute Privacy Terms of Use Trademarks \u00a9 Microsoft 2025", "url": "https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp", "position": {"start": 3150, "end": 3226}, "timestamp": "2025-04-20T23:41:40.112554", "chunk_id": "https_learn_microsoft_com_en-us_azure_azure-functions_functions-best-practices?tabs=csharp_3150", "title": "Azure Functions best practices | Microsoft Learn"}, {"text": "Session 10 - Deployment on Edge Devices You need to have JavaScript enabled in order to access this site. Global Navigation Menu [\u203b] EMLO 2.0 Session 10 - Deployment on Edge Devices Skip To Content Dashboard Account Dashboard Courses Calendar Inbox History 5 unread release notes. 5 Help Close My Dashboard [\u203b] EMLO 2.0 Assignments Session 10 - Deployment on Edge Devices Home Assignments Discussions Grades People Pages Files Syllabus Modules Collaborations Lucid (Whiteboard) Session 10 - Deployment on Edge Devices Start Assignment Due Nov 21, 2022 by 11:59pm Points 2,000 Submitting a text entry box Deployment on Raspberry Pi https://github.com/terryky/tflite_gles_app Links to an external site. Building the tflite .so files This can be done on a Host Machine with Ubuntu 18.04/20.04 (Docker is fine) You\u2019ll have a bad time if you use gcc 11.4 \ud83d\ude1e gcc 11.1.0 is fine Install Bazel 3.1.0 wget <https://github.com/bazelbuild/bazel/releases/download/3.1.0/bazel-3.1.0-installer-linux-x86_64.sh> chmod 755 bazel-3.1.0-installer-linux-x86_64.sh sudo ./bazel-3.1.0-installer-linux-x86_64.sh Build mkdir ~/work cd ~/work git clone <https://github.com/terryky/tflite_gles_app.git> ./tflite_gles_app/tools/scripts/tf2.4/build_libtflite_r2.4_aarch64.sh We need two files libtensorflowlite_gpu_delegate.so libtensorflowlite.so You can find the .so files here: https://drive.google.com/drive/folders/1DA4ImEbagPm6uEncU8bmRRZw8ROC8a5J?usp=sharing Links to an external site. or here if you built it yourself tensorflow_r2.4/bazel-bin/tensorflow/lite/delegates/gpu/libtensorflowlite_gpu_delegate.so tensorflow_r2.4/bazel-bin/tensorflow/lite/libtensorflowlite.so Copy these .so files to RPI/Jetson, into ~/lib folder nanodev@nanodev2:~$ ls ~/lib libtensorflowlite_gpu_delegate.so", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-20T23:42:51.866432", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_0", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "Build mkdir ~/work cd ~/work git clone <https://github.com/terryky/tflite_gles_app.git> ./tflite_gles_app/tools/scripts/tf2.4/build_libtflite_r2.4_aarch64.sh We need two files libtensorflowlite_gpu_delegate.so libtensorflowlite.so You can find the .so files here: https://drive.google.com/drive/folders/1DA4ImEbagPm6uEncU8bmRRZw8ROC8a5J?usp=sharing Links to an external site. or here if you built it yourself tensorflow_r2.4/bazel-bin/tensorflow/lite/delegates/gpu/libtensorflowlite_gpu_delegate.so tensorflow_r2.4/bazel-bin/tensorflow/lite/libtensorflowlite.so Copy these .so files to RPI/Jetson, into ~/lib folder nanodev@nanodev2:~$ ls ~/lib libtensorflowlite_gpu_delegate.so libtensorflowlite.so On Jetson/RPi machine we need to setup the libraries required to run our GLES + TFLITE Application mkdir ~/EMLO cd EMLO git clone -b r2.4 <https://github.com/tensorflow/tensorflow.git> cd tensorflow ./tensorflow/lite/tools/make/download_dependencies.sh sudo apt install libgles2-mesa-dev libdrm-dev cd ~/EMLO git clone <https://github.com/terryky/tflite_gles_app.git> cd tflite_gles_app/gl2blazeface modify Makefile slightly Set TENSORFLOW_DIR = /home/nanodev/EMLO/tensorflow Now Build ! On Jetson Nano make -j4 TARGET_ENV=jetson_nano TFLITE_DELEGATE=GPU_DELEGATEV2 On Raspberry Pi make -j4 TARGET_ENV=raspi4 Set the loader .so files path to include our lib folder export LD_LIBRARY_PATH=~/lib:$LD_LIBRARY_PATH Run ./gl2blazeface If you ever face issues running an executable and it complains about .so files, first check if the loader is able to find the required runtime shared objects on your system nanodev@nanodev2:~/EMLO/tflite_gles_app/gl2blazeface$ ldd gl2blazeface linux-vdso.so.1 (0x0000007fb53a1000) libEGL.so.1 => /usr/lib/aarch64-linux-gnu/libEGL.so.1 (0x0000007fb529d000) libGLESv2.so.2 => /usr/lib/aarch64-linux-gnu/libGLESv2.so.2 (0x0000007fb5267000) libX11.so.6 => /usr/lib/aarch64-linux-gnu/libX11.so.6 (0x0000007fb513d000) libdrm.so.2 => /usr/lib/aarch64-linux-gnu/libdrm.so.2 (0x0000007fb5106000) libtensorflowlite.so => /home/nanodev/lib/libtensorflowlite.so (0x0000007fb4de1000) libtensorflowlite_gpu_delegate.so => /home/nanodev/lib/libtensorflowlite_gpu_delegate.so (0x0000007fb4a24000) libstdc++.so.6 => /usr/lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000007fb4890000) libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000007fb47d7000) libgcc_s.so.1 =>", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-20T23:42:51.866445", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_150", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "able to find the required runtime shared objects on your system nanodev@nanodev2:~/EMLO/tflite_gles_app/gl2blazeface$ ldd gl2blazeface linux-vdso.so.1 (0x0000007fb53a1000) libEGL.so.1 => /usr/lib/aarch64-linux-gnu/libEGL.so.1 (0x0000007fb529d000) libGLESv2.so.2 => /usr/lib/aarch64-linux-gnu/libGLESv2.so.2 (0x0000007fb5267000) libX11.so.6 => /usr/lib/aarch64-linux-gnu/libX11.so.6 (0x0000007fb513d000) libdrm.so.2 => /usr/lib/aarch64-linux-gnu/libdrm.so.2 (0x0000007fb5106000) libtensorflowlite.so => /home/nanodev/lib/libtensorflowlite.so (0x0000007fb4de1000) libtensorflowlite_gpu_delegate.so => /home/nanodev/lib/libtensorflowlite_gpu_delegate.so (0x0000007fb4a24000) libstdc++.so.6 => /usr/lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000007fb4890000) libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000007fb47d7000) libgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000007fb47b3000) libpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000007fb4787000) libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000007fb462e000) /lib/ld-linux-aarch64.so.1 (0x0000007fb5375000) libGLdispatch.so.0 => /usr/lib/aarch64-linux-gnu/libGLdispatch.so.0 (0x0000007fb4502000) libdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000007fb44ed000) libxcb.so.1 => /usr/lib/aarch64-linux-gnu/libxcb.so.1 (0x0000007fb44bd000) libnvrm.so => /usr/lib/aarch64-linux-gnu/tegra/libnvrm.so (0x0000007fb4489000) libnvll.so => /usr/lib/aarch64-linux-gnu/tegra/libnvll.so (0x0000007fb446c000) libnvos.so => /usr/lib/aarch64-linux-gnu/tegra/libnvos.so (0x0000007fb444e000) librt.so.1 => /lib/aarch64-linux-gnu/librt.so.1 (0x0000007fb4437000) libXau.so.6 => /usr/lib/aarch64-linux-gnu/libXau.so.6 (0x0000007fb4424000) libXdmcp.so.6 => /usr/lib/aarch64-linux-gnu/libXdmcp.so.6 (0x0000007fb440f000) libnvrm_graphics.so => /usr/lib/aarch64-linux-gnu/tegra/libnvrm_graphics.so (0x0000007fb43ef000) libnvdc.so => /usr/lib/aarch64-linux-gnu/tegra/libnvdc.so (0x0000007fb43d2000) libbsd.so.0 => /lib/aarch64-linux-gnu/libbsd.so.0 (0x0000007fb43b0000) libnvimp.so => /usr/lib/aarch64-linux-gnu/tegra/libnvimp.so (0x0000007fb439b000) everything looks good ! Enabling Video Support sudo apt install libavcodec-dev libavdevice-dev libavfilter-dev libavformat-dev libavresample-dev libavutil-dev make -j4 TARGET_ENV=jetson_nano TFLITE_DELEGATE=GPU_DELEGATEV2 ENABLE_VDEC=true You can take videos from here https://www.pexels.com/search/videos/people Links to an external site. We are running at about 30 fps ! that is realtime ! this is face detection + 5 landmarks Deployment on Nvidia dGPU/Jetson DeepStream Architecture GStreamer Gstreamer Tutorial: https://github.com/satyajitghana/gstreamer-tutorial Links to an external site. https://gstreamer.freedesktop.org/documentation/tutorials/index.html Links to an external site. Threading gst-launch-1.0 https://gstreamer.freedesktop.org/documentation/tutorials/basic/gstreamer-tools.html Links to an external site. DeepStream Sample Apps: https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html Links", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-20T23:42:51.866450", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_300", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "at about 30 fps ! that is realtime ! this is face detection + 5 landmarks Deployment on Nvidia dGPU/Jetson DeepStream Architecture GStreamer Gstreamer Tutorial: https://github.com/satyajitghana/gstreamer-tutorial Links to an external site. https://gstreamer.freedesktop.org/documentation/tutorials/index.html Links to an external site. Threading gst-launch-1.0 https://gstreamer.freedesktop.org/documentation/tutorials/basic/gstreamer-tools.html Links to an external site. DeepStream Sample Apps: https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html Links to an external site. Deepstream Tutorial: https://github.com/satyajitghana/deepstream-tutorial Links to an external site. OpenCV with Gstreamer: https://github.com/mad4ms/python-opencv-gstreamer-examples Links to an external site. Setup VNC on Jetson sudo apt update sudo apt install vino mkdir -p ~/.config/autostart cp /usr/share/applications/vino-server.desktop ~/.config/autostart gsettings set org.gnome.Vino prompt-enabled false gsettings set org.gnome.Vino require-encryption false gsettings set org.gnome.Vino authentication-methods \"['vnc']\" gsettings set org.gnome.Vino vnc-password $(echo -n 'nanodev'|base64) sudo vim /etc/X11/xorg.conf Section \"Screen\" Identifier \"Default Screen\" Monitor \"Configured Monitor\" Device \"Tegra0\" SubSection \"Display\" Depth 24 Virtual 1280 800 EndSubSection EndSection sudo reboot Jetson Stats sudo apt-get install python3-pip sudo -H pip3 install -U jetson-stats sudo systemctl restart jetson_stats.service Installing DeepStream sudo apt install \\ libssl1.1 \\ libgstreamer1.0-0 \\ gstreamer1.0-tools \\ gstreamer1.0-plugins-good \\ gstreamer1.0-plugins-bad \\ gstreamer1.0-plugins-ugly \\ gstreamer1.0-libav \\ libgstreamer-plugins-base1.0-dev \\ libgstrtspserver-1.0-0 \\ libjansson4 \\ libyaml-cpp-dev Download the below tar file https://developer.nvidia.com/deepstream_sdk_v6.0.1_jetson.tbz2 sudo tar -xvf deepstream_sdk_v6.0.1_jetson.tbz2 -C / cd /opt/nvidia/deepstream/deepstream-6.0 sudo ./install.sh sudo ldconfig Lets try deepstream now !", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-20T23:42:51.866454", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_450", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "Installing DeepStream sudo apt install \\ libssl1.1 \\ libgstreamer1.0-0 \\ gstreamer1.0-tools \\ gstreamer1.0-plugins-good \\ gstreamer1.0-plugins-bad \\ gstreamer1.0-plugins-ugly \\ gstreamer1.0-libav \\ libgstreamer-plugins-base1.0-dev \\ libgstrtspserver-1.0-0 \\ libjansson4 \\ libyaml-cpp-dev Download the below tar file https://developer.nvidia.com/deepstream_sdk_v6.0.1_jetson.tbz2 sudo tar -xvf deepstream_sdk_v6.0.1_jetson.tbz2 -C / cd /opt/nvidia/deepstream/deepstream-6.0 sudo ./install.sh sudo ldconfig Lets try deepstream now ! \ud83d\udd25 cd /opt/nvidia/deepstream/deepstream-6.0/samples/configs/deepstream-app Create a file test-deepstream.txt [application] enable-perf-measurement=1 perf-measurement-interval-sec=5 #gie-kitti-output-dir=streamscl [tiled-display] enable=1 rows=2 columns=2 width=1280 height=720 gpu-id=0 #(0): nvbuf-mem-default - Default memory allocated, specific to particular platform #(1): nvbuf-mem-cuda-pinned - Allocate Pinned/Host cuda memory, applicable for Tesla #(2): nvbuf-mem-cuda-device - Allocate Device cuda memory, applicable for Tesla #(3): nvbuf-mem-cuda-unified - Allocate Unified cuda memory, applicable for Tesla #(4): nvbuf-mem-surface-array - Allocate Surface Array memory, applicable for Jetson nvbuf-memory-type=0 [source0] enable=1 #Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP type=3 uri=file://../../streams/sample_1080p_h264.mp4 num-sources=2 #drop-frame-interval=2 gpu-id=0 # (0): memtype_device - Memory type Device # (1): memtype_pinned - Memory type Host Pinned # (2): memtype_unified - Memory type Unified cudadec-memtype=0 [source1] enable=1 #Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP type=3 uri=file://../../streams/sample_1080p_h264.mp4 num-sources=2 gpu-id=0 # (0): memtype_device - Memory type Device # (1): memtype_pinned - Memory type Host Pinned # (2): memtype_unified - Memory type Unified cudadec-memtype=0 [sink0] enable=1 #Type - 1=FakeSink 2=EglSink 3=File type=2 sync=1 source-id=0", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-20T23:42:51.866459", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_600", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "- Memory type Unified cudadec-memtype=0 [source1] enable=1 #Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP type=3 uri=file://../../streams/sample_1080p_h264.mp4 num-sources=2 gpu-id=0 # (0): memtype_device - Memory type Device # (1): memtype_pinned - Memory type Host Pinned # (2): memtype_unified - Memory type Unified cudadec-memtype=0 [sink0] enable=1 #Type - 1=FakeSink 2=EglSink 3=File type=2 sync=1 source-id=0 gpu-id=0 nvbuf-memory-type=0 [sink1] enable=0 type=3 #1=mp4 2=mkv container=1 #1=h264 2=h265 codec=1 #encoder type 0=Hardware 1=Software enc-type=0 sync=0 #iframeinterval=10 bitrate=2000000 #H264 Profile - 0=Baseline 2=Main 4=High #H265 Profile - 0=Main 1=Main10 profile=0 output-file=out.mp4 source-id=0 [sink2] enable=0 #Type - 1=FakeSink 2=EglSink 3=File 4=RTSPStreaming type=4 #1=h264 2=h265 codec=1 #encoder type 0=Hardware 1=Software enc-type=0 sync=0 bitrate=4000000 #H264 Profile - 0=Baseline 2=Main 4=High #H265 Profile - 0=Main 1=Main10 profile=0 # set below properties in case of RTSPStreaming rtsp-port=8554 udp-port=5400 [osd] enable=1 gpu-id=0 border-width=1 text-size=15 text-color=1;1;1;1; text-bg-color=0.3;0.3;0.3;1 font=Serif show-clock=0 clock-x-offset=800 clock-y-offset=820 clock-text-size=12 clock-color=1;0;0;0 nvbuf-memory-type=0 [streammux] gpu-id=0 ##Boolean property to inform muxer that sources are live live-source=0 batch-size=30 ##time out in usec, to wait after the first buffer is available ##to push the batch even if the complete batch is not formed batched-push-timeout=40000 ## Set muxer output width and height width=1920 height=1080 ##Enable to maintain aspect ratio wrt source, and allow black borders, works ##along with width,", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-20T23:42:51.866464", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_750", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "batch-size=30 ##time out in usec, to wait after the first buffer is available ##to push the batch even if the complete batch is not formed batched-push-timeout=40000 ## Set muxer output width and height width=1920 height=1080 ##Enable to maintain aspect ratio wrt source, and allow black borders, works ##along with width, height properties enable-padding=0 nvbuf-memory-type=0 ## If set to TRUE, system timestamp will be attached as ntp timestamp ## If set to FALSE, ntp timestamp from rtspsrc, if available, will be attached # attach-sys-ts-as-ntp=1 # config-file property is mandatory for any gie section. # Other properties are optional and if set will override the properties set in # the infer config file. [primary-gie] enable=1 gpu-id=0 #model-engine-file=../../models/Primary_Detector/resnet10.caffemodel_b30_gpu0_int8.engine model-engine-file=/opt/nvidia/deepstream/deepstream-6.0/samples/models/Primary_Detector/resnet10.caffemodel_b30_gpu0_fp16.engine #Required to display the PGIE labels, should be added even when using config-file #property batch-size=30 #Required by the app for OSD, not a plugin property bbox-border-color0=1;0;0;1 bbox-border-color1=0;1;1;1 bbox-border-color2=0;0;1;1 bbox-border-color3=0;1;0;1 interval=0 #Required by the app for SGIE, when used along with config-file property gie-unique-id=1 nvbuf-memory-type=0 config-file=config_infer_primary.txt [tests] file-loop=0 This is a single Primary GIE inference export DISPLAY=:0 deepstream-app -c test-deepstream.txt First a TensorRT engine file will be created, everytime there is change is batch size / type of gpu / change of int8 or fp16", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-20T23:42:51.866468", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_900", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "for SGIE, when used along with config-file property gie-unique-id=1 nvbuf-memory-type=0 config-file=config_infer_primary.txt [tests] file-loop=0 This is a single Primary GIE inference export DISPLAY=:0 deepstream-app -c test-deepstream.txt First a TensorRT engine file will be created, everytime there is change is batch size / type of gpu / change of int8 or fp16 then this engine will be regenerated. Then the pipeline will start You can see the FPS we are running at **PERF: 13.53 (13.39) 13.83 (13.46) 13.53 (13.39) 13.53 (13.39) that 54 fps in total ! Let\u2019s try to chain models with PGIE and Secondary GIE Models deepstream-app -c source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt Lets see the config txt file [application] enable-perf-measurement=1 perf-measurement-interval-sec=5 #gie-kitti-output-dir=streamscl [tiled-display] enable=1 rows=2 columns=2 width=1280 height=720 gpu-id=0 #(0): nvbuf-mem-default - Default memory allocated, specific to particular platform #(1): nvbuf-mem-cuda-pinned - Allocate Pinned/Host cuda memory applicable for Tesla #(2): nvbuf-mem-cuda-device - Allocate Device cuda memory applicable for Tesla #(3): nvbuf-mem-cuda-unified - Allocate Unified cuda memory applicable for Tesla #(4): nvbuf-mem-surface-array - Allocate Surface Array memory, applicable for Jetson nvbuf-memory-type=0 [source0] enable=1 #Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP type=3 uri=file://../../streams/sample_1080p_h264.mp4 num-sources=4 #drop-frame-interval=2 gpu-id=0 # (0): memtype_device - Memory type Device # (1): memtype_pinned - Memory type Host Pinned # (2): memtype_unified - Memory", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-20T23:42:51.866472", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_1050", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "Unified cuda memory applicable for Tesla #(4): nvbuf-mem-surface-array - Allocate Surface Array memory, applicable for Jetson nvbuf-memory-type=0 [source0] enable=1 #Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP type=3 uri=file://../../streams/sample_1080p_h264.mp4 num-sources=4 #drop-frame-interval=2 gpu-id=0 # (0): memtype_device - Memory type Device # (1): memtype_pinned - Memory type Host Pinned # (2): memtype_unified - Memory type Unified cudadec-memtype=0 [sink0] enable=1 #Type - 1=FakeSink 2=EglSink 3=File type=2 sync=1 source-id=0 gpu-id=0 nvbuf-memory-type=0 [sink1] enable=0 type=3 #1=mp4 2=mkv container=1 #1=h264 2=h265 codec=1 #encoder type 0=Hardware 1=Software enc-type=0 sync=0 #iframeinterval=10 bitrate=2000000 #H264 Profile - 0=Baseline 2=Main 4=High #H265 Profile - 0=Main 1=Main10 profile=0 output-file=out.mp4 source-id=0 [sink2] enable=0 #Type - 1=FakeSink 2=EglSink 3=File 4=RTSPStreaming type=4 #1=h264 2=h265 codec=1 #encoder type 0=Hardware 1=Software enc-type=0 sync=0 bitrate=4000000 #H264 Profile - 0=Baseline 2=Main 4=High #H265 Profile - 0=Main 1=Main10 profile=0 # set below properties in case of RTSPStreaming rtsp-port=8554 udp-port=5400 [osd] enable=1 gpu-id=0 border-width=1 text-size=15 text-color=1;1;1;1; text-bg-color=0.3;0.3;0.3;1 font=Serif show-clock=0 clock-x-offset=800 clock-y-offset=820 clock-text-size=12 clock-color=1;0;0;0 nvbuf-memory-type=0 [streammux] gpu-id=0 ##Boolean property to inform muxer that sources are live live-source=0 buffer-pool-size=4 batch-size=4 ##time out in usec, to wait after the first buffer is available ##to push the batch even if the complete batch is not formed batched-push-timeout=40000 ## Set muxer output width and height width=1920 height=1080 ##Enable", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-20T23:42:51.866476", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_1200", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "nvbuf-memory-type=0 [streammux] gpu-id=0 ##Boolean property to inform muxer that sources are live live-source=0 buffer-pool-size=4 batch-size=4 ##time out in usec, to wait after the first buffer is available ##to push the batch even if the complete batch is not formed batched-push-timeout=40000 ## Set muxer output width and height width=1920 height=1080 ##Enable to maintain aspect ratio wrt source, and allow black borders, works ##along with width, height properties enable-padding=0 nvbuf-memory-type=0 ## If set to TRUE, system timestamp will be attached as ntp timestamp ## If set to FALSE, ntp timestamp from rtspsrc, if available, will be attached # attach-sys-ts-as-ntp=1 # config-file property is mandatory for any gie section. # Other properties are optional and if set will override the properties set in # the infer config file. [primary-gie] enable=1 gpu-id=0 model-engine-file=/opt/nvidia/deepstream/deepstream-6.0/samples/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_fp16.engine batch-size=4 #Required by the app for OSD, not a plugin property bbox-border-color0=1;0;0;1 bbox-border-color1=0;1;1;1 bbox-border-color2=0;0;1;1 bbox-border-color3=0;1;0;1 interval=0 gie-unique-id=1 nvbuf-memory-type=0 config-file=config_infer_primary.txt [tracker] enable=1 # For NvDCF and DeepSORT tracker, tracker-width and tracker-height must be a multiple of 32, respectively tracker-width=640 tracker-height=384 ll-lib-file=/opt/nvidia/deepstream/deepstream-6.0/lib/libnvds_nvmultiobjecttracker.so # ll-config-file required to set different tracker types # ll-config-file=config_tracker_IOU.yml ll-config-file=config_tracker_NvDCF_perf.yml # ll-config-file=config_tracker_NvDCF_accuracy.yml # ll-config-file=config_tracker_DeepSORT.yml gpu-id=0 enable-batch-process=1 enable-past-frame=1 display-tracking-id=1 [secondary-gie0] enable=1 model-engine-file=/opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_fp16.engine gpu-id=0 batch-size=16 gie-unique-id=4 operate-on-gie-id=1 operate-on-class-ids=0; config-file=config_infer_secondary_vehicletypes.txt [secondary-gie1] enable=1 model-engine-file=/opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_fp16.engine", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-20T23:42:51.866481", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_1350", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "# For NvDCF and DeepSORT tracker, tracker-width and tracker-height must be a multiple of 32, respectively tracker-width=640 tracker-height=384 ll-lib-file=/opt/nvidia/deepstream/deepstream-6.0/lib/libnvds_nvmultiobjecttracker.so # ll-config-file required to set different tracker types # ll-config-file=config_tracker_IOU.yml ll-config-file=config_tracker_NvDCF_perf.yml # ll-config-file=config_tracker_NvDCF_accuracy.yml # ll-config-file=config_tracker_DeepSORT.yml gpu-id=0 enable-batch-process=1 enable-past-frame=1 display-tracking-id=1 [secondary-gie0] enable=1 model-engine-file=/opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_fp16.engine gpu-id=0 batch-size=16 gie-unique-id=4 operate-on-gie-id=1 operate-on-class-ids=0; config-file=config_infer_secondary_vehicletypes.txt [secondary-gie1] enable=1 model-engine-file=/opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_fp16.engine batch-size=16 gpu-id=0 gie-unique-id=5 operate-on-gie-id=1 operate-on-class-ids=0; config-file=config_infer_secondary_carcolor.txt [secondary-gie2] enable=1 model-engine-file=/opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_CarMake/resnet18.caffemodel_b16_gpu0_fp16.engine batch-size=16 gpu-id=0 gie-unique-id=6 operate-on-gie-id=1 operate-on-class-ids=0; config-file=config_infer_secondary_carmake.txt [tests] file-loop=0 PGIE /opt/nvidia/deepstream/deepstream-6.0/samples/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_fp16.engine SGIE CarMake /opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_CarMake/resnet18.caffemodel_b16_gpu0_fp16.engine CarColor /opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_fp16.engine VehicleType /opt/nvidia/deepstream/deepstream-6.0/samples/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_fp16.engine Tracker https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvtracker.html Links to an external site. You can see all the available config options here: https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_ref_app_deepstream.html Links to an external site. You can see the entire source code of deepstream-app here: /opt/nvidia/deepstream/deepstream-6.0/sources/apps/sample_apps/deepstream-app It is entirely written with gstreamer pipeline ! People Count Example App: https://github.com/MACNICA-CLAVIS-NV/deepstream-people-count Links to an external site. If you every want to make a custom plugin this is a great starting place: Gst-dsexample https://docs.nvidia.com/metropolis/deepstream/5.0DP/dev-guide/index.html#page/DeepStream_Development_Guide/deepstream_custom_plugin.html Links to an external site. /opt/nvidia/deepstream/deepstream-6.0/sources/gst-plugins/gst-dsexample YOLO Deepstream git clone <https://github.com/marcoslucianops/DeepStream-Yolo.git> cd DeepStream-Yolo/ Make the custom Yolo Implementation CUDA_VER=10.2 make -C nvdsinfer_custom_impl_Yolo YOLOV7 We first need to convert the weights of YOLOV7 to the format that the YOLO Deepstream plugin can understand. This conversion can be done on any system. Download weights from https://github.com/WongKinYiu/yolov7/releases/tag/v0.1 Links to", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-20T23:42:51.866485", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_1500", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "YOLO Deepstream git clone <https://github.com/marcoslucianops/DeepStream-Yolo.git> cd DeepStream-Yolo/ Make the custom Yolo Implementation CUDA_VER=10.2 make -C nvdsinfer_custom_impl_Yolo YOLOV7 We first need to convert the weights of YOLOV7 to the format that the YOLO Deepstream plugin can understand. This conversion can be done on any system. Download weights from https://github.com/WongKinYiu/yolov7/releases/tag/v0.1 Links to an external site. git clone <https://github.com/WongKinYiu/yolov7.git> cd yolov7 pip install -r requirements.txt wget <https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt> python gen_wts_yoloV7.py -w yolov7-tiny.pt Copy the generated cfg and wts files to the DeepStream-Yolo folder on your Jetson I\u2019ve uploaded the converted weights for yolov7-tiny: https://drive.google.com/drive/folders/1DA4ImEbagPm6uEncU8bmRRZw8ROC8a5J?usp=sharing Links to an external site. config_infer_primary_yoloV7.txt [property] gpu-id=0 net-scale-factor=0.0039215697906911373 model-color-format=0 custom-network-config=/home/nanodev/10-deployment-on-edge-device/yolov7-tiny.cfg model-file=/home/nanodev/10-deployment-on-edge-device/yolov7-tiny.wts model-engine-file=model_b1_gpu0_fp32.engine #int8-calib-file=calib.table labelfile-path=labels.txt batch-size=1 network-mode=0 num-detected-classes=80 interval=0 gie-unique-id=1 process-mode=1 network-type=0 cluster-mode=2 maintain-aspect-ratio=1 parse-bbox-func-name=NvDsInferParseYolo custom-lib-path=nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so engine-create-func-name=NvDsInferYoloCudaEngineGet [class-attrs-all] nms-iou-threshold=0.45 pre-cluster-threshold=0.25 topk=300 deepstream_app_config.txt [application] enable-perf-measurement=1 perf-measurement-interval-sec=5 [tiled-display] enable=1 rows=1 columns=1 width=1280 height=720 gpu-id=0 nvbuf-memory-type=0 [source0] enable=1 type=3 uri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4 num-sources=1 gpu-id=0 cudadec-memtype=0 [sink0] enable=1 type=2 sync=0 gpu-id=0 nvbuf-memory-type=0 [osd] enable=1 gpu-id=0 border-width=5 text-size=15 text-color=1;1;1;1; text-bg-color=0.3;0.3;0.3;1 font=Serif show-clock=0 clock-x-offset=800 clock-y-offset=820 clock-text-size=12 clock-color=1;0;0;0 nvbuf-memory-type=0 [streammux] gpu-id=0 live-source=0 batch-size=1 batched-push-timeout=40000 width=1920 height=1080 enable-padding=0 nvbuf-memory-type=0 [primary-gie] enable=1 gpu-id=0 gie-unique-id=1 nvbuf-memory-type=0 config-file=config_infer_primary_yoloV7.txt [tests] file-loop=0 export DISPLAY=:0 deepstream-app -c deepstream_app_config.txt First an TensorRT Engine file will be generated -rw-rw-r-- 1 nanodev nanodev 51M Nov 8 20:52 model_b1_gpu0_fp32.engine This", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 1650, "end": 1849}, "timestamp": "2025-04-20T23:42:51.866490", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_1650", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "text-size=15 text-color=1;1;1;1; text-bg-color=0.3;0.3;0.3;1 font=Serif show-clock=0 clock-x-offset=800 clock-y-offset=820 clock-text-size=12 clock-color=1;0;0;0 nvbuf-memory-type=0 [streammux] gpu-id=0 live-source=0 batch-size=1 batched-push-timeout=40000 width=1920 height=1080 enable-padding=0 nvbuf-memory-type=0 [primary-gie] enable=1 gpu-id=0 gie-unique-id=1 nvbuf-memory-type=0 config-file=config_infer_primary_yoloV7.txt [tests] file-loop=0 export DISPLAY=:0 deepstream-app -c deepstream_app_config.txt First an TensorRT Engine file will be generated -rw-rw-r-- 1 nanodev nanodev 51M Nov 8 20:52 model_b1_gpu0_fp32.engine This is the MOST OPTIMIZED version of the model specifically made for your platform gpu. Sometimes if you system has DLA cores, then this engine has optimization for that as well. **PERF: 7.04 (7.05) **PERF: 7.49 (7.13) **PERF: 7.17 (7.11) **PERF: 7.30 (7.15) **PERF: 7.49 (7.17) **PERF: 7.65 (7.23) **PERF: 7.98 (7.30) **PERF: 7.94 (7.34) **PERF: 7.83 (7.37) **PERF: 7.88 (7.42) **PERF: 7.74 (7.43) **PERF: 7.61 (7.44) Notice that NVDEC is now active ! Also the GPU usage is at 99%, and this stays at that ALL THE TIME. This also means there arn\u2019t any CPU to GPU movements, which usually is the bottleneck. Jetson Clocks Jetson provides the jetson_clocks ******script to maximize Jetson performance by setting static max frequency to CPU, GPU, and EMC clocks sudo jetson_clocks **PERF: 8.44 (8.26) **PERF: 8.35 (8.33) **PERF: 8.39 (8.36) **PERF: 8.33 (8.37) **PERF: 8.35 (8.33) VPI https://developer.nvidia.com/embedded/vpi Links to an external site. More", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 1800, "end": 1999}, "timestamp": "2025-04-20T23:42:51.866494", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_1800", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "is the bottleneck. Jetson Clocks Jetson provides the jetson_clocks ******script to maximize Jetson performance by setting static max frequency to CPU, GPU, and EMC clocks sudo jetson_clocks **PERF: 8.44 (8.26) **PERF: 8.35 (8.33) **PERF: 8.39 (8.36) **PERF: 8.33 (8.37) **PERF: 8.35 (8.33) VPI https://developer.nvidia.com/embedded/vpi Links to an external site. More references for TensorRT Object Detection at 2350 fps with TensorRT: https://paulbridger.com/posts/tensorrt-object-detection-quantized Links to an external site. Studio Recording There was a lot of construction noise this time :( GM Recording Assignment Run YOLOV7, YOLOV7-Tiny on your own video (30s) with Deepstream on Jetson Nano Save the video to a file and upload it to YouTube/LinkedIn submit the link of both yolov7 and yolov7-tiny Submit the FPS you were able to achieve Create Groups and Submit your SSH Public Key here: https://docs.google.com/spreadsheets/d/10FM4cgOMRfEdQLlv0DXyK-U6KntDNDKs-y4ICvdGGqk/edit?usp=sharing Links to an external site. You can access the Jetson Nano using ssh -p 7022 nanodev@emlo.theschoolof.ai and for Display Access ssh -p 7022 -L 5900:127.0.0.1:5900 -C -N -l nanodev emlo.theschoolof.ai Now you can use RealVNC Client and connect to localhost:5900 1669055340 11/21/2022 11:59pm Text Entry Copy and paste or type your submission right here. Cancel Submit Assignment Description Please include a description Long Description Cancel Update Criterion Additional Comments:", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 1950, "end": 2149}, "timestamp": "2025-04-20T23:42:51.866498", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_1950", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "Access ssh -p 7022 -L 5900:127.0.0.1:5900 -C -N -l nanodev emlo.theschoolof.ai Now you can use RealVNC Client and connect to localhost:5900 1669055340 11/21/2022 11:59pm Text Entry Copy and paste or type your submission right here. Cancel Submit Assignment Description Please include a description Long Description Cancel Update Criterion Additional Comments: Cancel Update Comments Additional Comments: Rating Score Rating max score to > pts Rating Title Please include a rating title Rating Description Cancel Update Rating Rubric Can't change a rubric once you've started using it. Find a Rubric Title: Find Rubric Please include a title Title You've already rated students with this rubric. Any major changes could affect their assessment results. Title Criteria Ratings Pts Edit criterion description Delete criterion row This criterion is linked to a Learning Outcome Description of criterion Range threshold: 5 pts Edit rating Delete rating 5 to > 0 pts Full Marks blank Edit rating Delete rating 0 to > 0 pts No Marks blank_2 This area will be used by the assessor to leave comments related to this criterion. pts / 5 pts -- Edit criterion description Delete criterion row This criterion is linked to a Learning Outcome Description of criterion Range threshold: 5", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 2100, "end": 2299}, "timestamp": "2025-04-20T23:42:51.866503", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_2100", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "Delete rating 0 to > 0 pts No Marks blank_2 This area will be used by the assessor to leave comments related to this criterion. pts / 5 pts -- Edit criterion description Delete criterion row This criterion is linked to a Learning Outcome Description of criterion Range threshold: 5 pts Edit rating Delete rating 5 to > 0 pts Full Marks blank Edit rating Delete rating 0 to > 0 pts No Marks blank_2 This area will be used by the assessor to leave comments related to this criterion. pts / 5 pts -- Total Points: 5 out of 5 I'll write free-form comments when assessing students Remove points from rubric Don't post Outcomes results to Learning Mastery Gradebook Use this rubric for assignment grading Hide score total for assessment results Cancel Create Rubric Previous Next b4842d7c-fe0f-44e2-99a8-099d5dac74fb Equella is a shared content repository that organizations can use to easily track and reuse content. This OER repository is a collection of free resources provided by Equella. Support Session 09 - Deployment on Accelerators and Serverless Session 11 - Drift Drift Drift", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 2250, "end": 2430}, "timestamp": "2025-04-20T23:42:51.866507", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_2250", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "track and reuse content. This OER repository is a collection of free resources provided by Equella. Support Session 09 - Deployment on Accelerators and Serverless Session 11 - Drift Drift Drift", "url": "https://canvas.instructure.com/courses/5120078/assignments/33616909?module_item_id=75210147", "position": {"start": 2400, "end": 2430}, "timestamp": "2025-04-20T23:42:51.866509", "chunk_id": "https_canvas_instructure_com_courses_5120078_assignments_33616909?module_item_id=75210147_2400", "title": "Session 10 - Deployment on Edge Devices"}, {"text": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5 3 B l u e 1 B r o w n Menu Lessons SoME Blog Extras Store FAQ Contact About Neural Networks But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5 Published Apr 1, 2024 Updated Mar 13, 2025 Lesson by Grant Sanderson Text adaptation by Justin Sun Source Code What is a GPT model? Formally speaking, a GPT is a Generative Pre-Trained Transformer . The first two words are self-explanatory: generative means the model generates new text; pre-trained means the model was trained on large amounts of data. What we will focus on is the transformer aspect of the language model, the main proponent of the recent boom in AI. What exactly is a Transformer? A transformer is a special kind of neural network, a Machine Learning Model . There are a wide variety of models that can be built using transformers : voice-to-text, text-to-voice, text-to-image, machine translation, and many more. The specific variant that we will focus on, which is the type that underlies tools like ChatGPT, will be a model trained to take in a piece", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-23T14:59:37.383279", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_0", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": ". There are a wide variety of models that can be built using transformers : voice-to-text, text-to-voice, text-to-image, machine translation, and many more. The specific variant that we will focus on, which is the type that underlies tools like ChatGPT, will be a model trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, then produce a prediction of what comes next, in the form of a probability distribution over all chunks of text that might follow. At first, predicting the next word might feel like a different goal from generating new text. But once you have a prediction model like this, one simple way to make this generate a longer piece is to give it an initial bit of text to work with, have it predict the next word, take a random sample from the distribution it just generated, then run it all again to make a new prediction based on all the text, including what it just added. This process of repeated prediction and sampling is essentially what\u2019s happening when you interact with ChatGPT and see it producing one word at a time. We'll first preview the transformer with a", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-23T14:59:37.383294", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_150", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "it all again to make a new prediction based on all the text, including what it just added. This process of repeated prediction and sampling is essentially what\u2019s happening when you interact with ChatGPT and see it producing one word at a time. We'll first preview the transformer with a high-level perspective. We\u2019ll spend much more time motivating, interpreting, and expanding on the details of each step, but in broad strokes, when one of these chatbots is generating text, here\u2019s what\u2019s going on under the hood. Tokens An input is first broken into small chunks that are known as tokens . For example, in the sentence: To date, the cleverest thinker of all time was ... The tokenization of this input would be: To| date|,| the| cle|ve|rest| thinker| of| all| time| was ... Each of these tokens is then associated with a vector, meaning some list of numbers. A common interpretation of these embeddings is that the coordinates of these vectors may somehow encode the meaning of each token. If you think of these vectors as giving coordinates in some high-dimensional space, words with similar meanings tend to land on vectors close to each other in that space. These steps", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-23T14:59:37.383300", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_300", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "interpretation of these embeddings is that the coordinates of these vectors may somehow encode the meaning of each token. If you think of these vectors as giving coordinates in some high-dimensional space, words with similar meanings tend to land on vectors close to each other in that space. These steps are pre-processing steps that occur before anything enters the transformer itself. Attention Block The encoded vectors then pass through an Attention Block where they communicate with each other to update their values based on context. For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model . The Attention Block is responsible for figuring out which words in the context are relevant to updating the meanings of other words and how exactly those meanings should be updated. Multilayer Perceptron(Feed-Forward Layer) Following the Attention Block, these vectors then pass through a Multilayer Perceptron, or Feed-Forward Layer. Here, the vectors don\u2019t talk to each other; they all go through the same operation in parallel. We\u2019ll talk later about how this step is a bit like asking a long list of questions about each vector and updating them", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-23T14:59:37.383307", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_450", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "vectors then pass through a Multilayer Perceptron, or Feed-Forward Layer. Here, the vectors don\u2019t talk to each other; they all go through the same operation in parallel. We\u2019ll talk later about how this step is a bit like asking a long list of questions about each vector and updating them based on the answers. After that, the vectors pass through another attention block, then another multilayer perceptron block, then another attention block, and so on, getting altered by many variants of these two operations interlaced with one another. A large number of layers like this is what puts the \"deep\" in deep learning. Computationally, all the operations in both blocks will look like a giant pile of matrix multiplications, and our goal will be to understand how to read the underlying matrices. After many iterations, all the information necessary to predict the next word needs to be encoded into the last vector of the sequence, which will go through one final computation to produce a probability distribution over all the possible chunks of text that might come next. What is a common interpretation of the coordinates of the vectors that are associated with the tokens? With that as a high-level", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-23T14:59:37.383311", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_600", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "last vector of the sequence, which will go through one final computation to produce a probability distribution over all the possible chunks of text that might come next. What is a common interpretation of the coordinates of the vectors that are associated with the tokens? With that as a high-level preview, in this lesson we will expand on the details of what happens both at the beginning and the end of the network. But first, it may help to review some of the background knowledge that would have been second nature to any machine learning engineer by the time transformers came around. Premise of Deep Learning Machine learning, broadly speaking, describes a body of methods where one uses data to determine the behavior of a program, as opposed to relying entirely on an explicitly encoded set of rules. For example, to write a function that takes in an image and produces a label, rather than explicitly defining a procedure for doing this in code, a machine learning model will consist of flexible parameters that determine its behavior, like a bunch of knobs and dials. The job of the engineer is not to set those dials, but to determine a procedure", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-23T14:59:37.383315", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_750", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "produces a label, rather than explicitly defining a procedure for doing this in code, a machine learning model will consist of flexible parameters that determine its behavior, like a bunch of knobs and dials. The job of the engineer is not to set those dials, but to determine a procedure to tune those parameters based on data. For example, an image-labeling program may be trained using a large set of images with known labels. For example, the simplest form of machine learning might be linear regression, where inputs and outputs are single numbers, such as the square footage of a house and its price. A linear regression finds the line of best fit through the data to predict future house prices. This line is determined by two parameters: the slope and the y-intercept, which are tuned to most closely match the data. Then for future houses, with unknown prices, the predicted price would be determined based on the value of this line over the given square footage. Deep learning is a subfield of Machine Learning, focused on a specific category of models known as Neural Networks. Needless to say, these can get dramatically more complicated than the simple linear regression", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-23T14:59:37.383319", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_900", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "predicted price would be determined based on the value of this line over the given square footage. Deep learning is a subfield of Machine Learning, focused on a specific category of models known as Neural Networks. Needless to say, these can get dramatically more complicated than the simple linear regression example. GPT-3, for example, had an astounding 175 billion parameters. However, simply giving a model a huge number of parameters does not guarantee that it will perform better. Models at such scales risk being either completely intractable to train, prone to overfitting, or both. Some deep learning models have shown a remarkable increase in capability as they size of the model and data it's trained on scale to enormous sizes. They are trained using an algorithm called backpropagation , but in order for this algorithm to work well at scale, these models have to adhere to a specific format. Understanding this format will help explain many of the choices in how transformers process language\u2014choices that might otherwise seem arbitrary. First , the input to the model has to be formatted as an array of real numbers. This could simply mean a list of numbers, a 2D array, or often higher-dimensional", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-23T14:59:37.383323", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_1050", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "this format will help explain many of the choices in how transformers process language\u2014choices that might otherwise seem arbitrary. First , the input to the model has to be formatted as an array of real numbers. This could simply mean a list of numbers, a 2D array, or often higher-dimensional arrays, where the general term used here is tensor . The input data is progressively transformed into many different layers, always structured like some array of numbers, until it reaches the final layer as the output. For example, the final layer in our next-token-prediction model is the probability distribution over all possible next tokens. In deep learning, the model parameters are almost always referred to as weights , because for the most part, the only way they interact with the data being processed is through weighted sums. Typically, you will find the weights packaged together in matrices, and instead of seeing weighted sums written out explicitly in the mathematics of such a model, you only see matrix-vector products. It represents the same idea, since each component in the output of a matrix-vector product looks like a weighted sum, it\u2019s just often conceptually cleaner to think about matrices filled with tunable", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-23T14:59:37.383327", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_1200", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "seeing weighted sums written out explicitly in the mathematics of such a model, you only see matrix-vector products. It represents the same idea, since each component in the output of a matrix-vector product looks like a weighted sum, it\u2019s just often conceptually cleaner to think about matrices filled with tunable parameters transforming vectors drawn from the data being processed. To prevent the entire model from being linear, there will typically also be some nonlinear functions sprinkled in between these matrix-vector products, such as the softmax operation we will see at the end of this article. For example, the 175 billion weights in GPT-3 are organized into just under 28,000 different matrices. Those matrices fall into 8 different categories, and we will step through each type to understand what it does. As we go through, it will be fun for us to reference the numbers from GPT-3 to count up exactly where those 175 billion parameters all come from. There's a risk of getting lost in the vast amount of numbers, and in the hopes of adding clarity we'll draw a sharp distinction between the weights of the model, colored in blue or red, and the data being processed, colored in", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-23T14:59:37.383331", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_1350", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "where those 175 billion parameters all come from. There's a risk of getting lost in the vast amount of numbers, and in the hopes of adding clarity we'll draw a sharp distinction between the weights of the model, colored in blue or red, and the data being processed, colored in grey. The weights are the actual brains of the model, learned during training and determining how it behaves. The data being processed encodes whatever specific input was fed into the model in a given instance, like an example snippet of text. The model parameters are commonly referred to as... Tensors Weights Non-Linear Functions Check Answer Embedding With all that as a foundation, let\u2019s dig into the text-processing example, starting with this first step of breaking up the input into little chunks and turning those chunks into vectors. We mentioned earlier how these little chunks are called tokens, which might be pieces of words or punctuation. For the sake of simplicity, we will pretend that the input is cleanly broken into words. Since we humans think in words, this will make it much easier to reference little examples to clarify each step. Embedding Matrix The model has a predefined vocabulary, some", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-23T14:59:37.383335", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_1500", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "pieces of words or punctuation. For the sake of simplicity, we will pretend that the input is cleanly broken into words. Since we humans think in words, this will make it much easier to reference little examples to clarify each step. Embedding Matrix The model has a predefined vocabulary, some list of all possible words, say 50,000 of them. The first matrix of the transformer, known as the embedding matrix , will have one column for each of these words. These columns determine what vector each word turns into in that first step. We label it as W E W_E W E \u200b , and like all the matrices we see, its values begin random, but they're going to be learned based on data. Turning words into vectors was common practice in machine learning long before transformers. This is often called embedding the word , which invites thinking of these vectors geometrically as points or directions in some space. Visualizing a list of three numbers as coordinates for a point in 3D space is no problem, but word embeddings tend to be very high dimensional. For GPT-3, they have 12,288 dimensions, and as you will see, it matters to work", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 1650, "end": 1849}, "timestamp": "2025-04-23T14:59:37.383339", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_1650", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "vectors geometrically as points or directions in some space. Visualizing a list of three numbers as coordinates for a point in 3D space is no problem, but word embeddings tend to be very high dimensional. For GPT-3, they have 12,288 dimensions, and as you will see, it matters to work in a space with lots of distinct directions. In the same way that you can take a 2D slice through 3D space and project points onto that slice, for the sake of visualizing word embeddings, we will do something analogous by choosing a 3D slice through the very high-dimensional space, projecting word vectors onto that, and displaying the result. Direction The big idea we need to understand here is that as a model tweaks and tunes its weights to decide how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in this space have meaning. Below, a simple word-to-vector model is running, and when I run a search for all words whose embeddings are closest to that of tower , they all generally have the same vibe. Another classic example of this is when the difference between the vectors for", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 1800, "end": 1999}, "timestamp": "2025-04-23T14:59:37.383342", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_1800", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "in this space have meaning. Below, a simple word-to-vector model is running, and when I run a search for all words whose embeddings are closest to that of tower , they all generally have the same vibe. Another classic example of this is when the difference between the vectors for woman and man is taken, which can be visualized as a vector in this space connecting the tip of one to the tip of the other. This difference is quite similar to the difference between king and queen . So, if the word for a female monarch was unknown, it could be found by taking king , adding the direction of woman minus man , and searching for the closest word embedding. At least, kind of. Despite this being the classic example, for the model I was playing with when making this video, the true embedding of queen is a little farther off than the difference would suggest, presumably because the way queen is used in training data is not merely a feminine version of a king. Family relations illustrate the idea better: The idea here is that it seems as if, during training, the model found it advantageous to", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 1950, "end": 2149}, "timestamp": "2025-04-23T14:59:37.383346", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_1950", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "little farther off than the difference would suggest, presumably because the way queen is used in training data is not merely a feminine version of a king. Family relations illustrate the idea better: The idea here is that it seems as if, during training, the model found it advantageous to choose embeddings such that one direction in this space encodes gender information. Another example of this would be taking the embedding of Italy , subtracting the embedding of Germany , and adding it to the embedding of Hitler . This results in something very close to the embedding of Mussolini . It\u2019s as if the model learned to associate some directions with Italian-ness and others with WWII Axis leaders. Dot Product One bit of mathematical intuition helpful to have in mind as we continue is how the dot product of two vectors can be thought of as measuring how well they align. Computationally, dot products involve multiplying all aligning components and adding the result. Geometrically, the dot product is positive when the vectors point in a similar direction, zero if they're perpendicular , and negative when they point in opposite directions. For example, suppose we wanted to test if the", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 2100, "end": 2299}, "timestamp": "2025-04-23T14:59:37.383350", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_2100", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "they align. Computationally, dot products involve multiplying all aligning components and adding the result. Geometrically, the dot product is positive when the vectors point in a similar direction, zero if they're perpendicular , and negative when they point in opposite directions. For example, suppose we wanted to test if the embedding of cats minus that of cat represents a kind of plurality direction in this space. To test this, you could take the dot product between this vector and various singular and plural nouns. When I did this for a simple word-to-vector model while making this video, it looked like the plural ones do indeed end up with consistently higher values than singular ones. Also, taking this dot product with the embeddings of the words one , two , three , and four , they give increasing values, as if it\u2019s quantitatively measuring how plural the model finds a given word. Again, how specifically each word gets embedded is learned using data. The embedding matrix, whose columns store the embedding of each word, is the first pile of weights in our model. Using the GPT-3 numbers, the vocabulary size is 50,257, and again, technically this consists not of words, per", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 2250, "end": 2449}, "timestamp": "2025-04-23T14:59:37.383354", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_2250", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "word. Again, how specifically each word gets embedded is learned using data. The embedding matrix, whose columns store the embedding of each word, is the first pile of weights in our model. Using the GPT-3 numbers, the vocabulary size is 50,257, and again, technically this consists not of words, per se, but different little chunks of text called tokens. The embedding dimension is 12,288, giving us 617,558,016 weights in total for this first step. Let\u2019s go add that to a running tally, remembering that by the end we should count up to 175 billion weights. In a vector space representing meanings, which of the following would you expect to be closest to the vector Embedding(Cat) - Embedding(Meow) + Embedding(Bark) Embedding(Tree) Embedding(Sound) Embedding(Dog) Check Answer Beyond words In the case of a transformer, we also want to think of the vectors in this embedding space as not merely representing individual words. For one thing, these embeddings will also encode information about the position of the word, but more importantly, they need to have the capacity to soak in context. For example, a vector that started its life as the embedding of the word \u201cking\u201d may progressively get tugged and pulled by", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 2400, "end": 2599}, "timestamp": "2025-04-23T14:59:37.383358", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_2400", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "For one thing, these embeddings will also encode information about the position of the word, but more importantly, they need to have the capacity to soak in context. For example, a vector that started its life as the embedding of the word \u201cking\u201d may progressively get tugged and pulled by the various blocks in the network to end up pointing in a much more nuanced direction that somehow encodes a king who lived in Scotland, who had achieved his post after murdering the previous king, who is being described in Shakespearean language, and so on. Think about our understanding of a word, like quill. Its meaning is clearly informed by its surroundings and context, whether it be a hedgehog quill or a type of pen. Sometimes, we may even include context from a long distance away. When putting together a model that is able to predict the next word, the goal is to somehow empower it to do the same thing: take in context efficiently. In that very first step, when you create the array of vectors based on the input text, each one is simply plucked out of this embedding matrix, and each one only encodes the meaning of", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 2550, "end": 2749}, "timestamp": "2025-04-23T14:59:37.383362", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_2550", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "goal is to somehow empower it to do the same thing: take in context efficiently. In that very first step, when you create the array of vectors based on the input text, each one is simply plucked out of this embedding matrix, and each one only encodes the meaning of the single word it's associated with. It's effectively a lookup table with no input from the surroundings. But as these vectors flow through the network, we should view the primary goal of this network as enabling each one of those vectors to soak up meaning that is more rich and specific than what mere individual words can represent. The network can only look at a fixed number of vectors at a time, known as its context size . GPT-3 was trained with a context size of 2048 tokens. So the data flowing through the network will look like this array of 2048 columns, each of which has around 12k dimensions. This context size limits how much text the transformer can incorporate to make its prediction of the next word, which is why long conversations with the early versions of ChatGPT often gave the feeling of the bot losing the thread", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 2700, "end": 2899}, "timestamp": "2025-04-23T14:59:37.383366", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_2700", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "of 2048 columns, each of which has around 12k dimensions. This context size limits how much text the transformer can incorporate to make its prediction of the next word, which is why long conversations with the early versions of ChatGPT often gave the feeling of the bot losing the thread of conversation. Unembedding We'll go into the details of the Attention Block in the next chapter, but first we'll skip ahead and talk about what happens at the very end of the Transformer. Remember, the desired output is a probability distribution over all possible chunks of text that might come next. For example, if the last word is Professor , and the context includes words like Harry Potter , and the immediately preceding is least favorite , and if we pretend that all tokens look like full words, a well-trained network would presumably assign a high number to the word Snape . Unembedding Matrix This process involves two steps. The first is to use another matrix that maps the very last vector in the context to a list of ~50,000 values, one for each token in the vocabulary, then there\u2019s a function that normalizes this into a probability distribution, called", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 2850, "end": 3049}, "timestamp": "2025-04-23T14:59:37.383370", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_2850", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "Snape . Unembedding Matrix This process involves two steps. The first is to use another matrix that maps the very last vector in the context to a list of ~50,000 values, one for each token in the vocabulary, then there\u2019s a function that normalizes this into a probability distribution, called softmax , which we\u2019ll talk more about in just a second. It might seem weird to only use the last embedding to make a prediction when there are thousands of other vectors in that last layer just sitting here with their own context-rich meanings. This has to do with how the training process turns out to be much more efficient if we use each vector in this final layer to simultaneously make a prediction for what comes immediately after it. We'll talk a lot more about the training process later. This is often called the unembedding matrix , and we'll give it the label W U W_U W U \u200b . Again, like all the weight matrices we\u2019ll see, its entries begin random but are tuned during the training process. Keeping score on our total number of parameters, the unembedding matrix has one row for each word in the vocabulary,", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 3000, "end": 3199}, "timestamp": "2025-04-23T14:59:37.383374", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_3000", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "give it the label W U W_U W U \u200b . Again, like all the weight matrices we\u2019ll see, its entries begin random but are tuned during the training process. Keeping score on our total number of parameters, the unembedding matrix has one row for each word in the vocabulary, giving 50,257 words, and each row has the same number of elements as the dimension of the embedding, giving 12,288 columns. It\u2019s very similar to the embedding matrix, just with the dimensions of the rows and columns swapped, so it adds another 617M parameters to the network, making our parameter count so far a little over a billion; a small but not insignificant fraction of the 175 billion that we'll end up with in total. If the model has a predefined vocabulary of 32,000 tokens and the dimension of the embedding matrix dimension is 10,000, the unembedding matrix has: 32,000 rows 320,000,000 tokens 10,000 rows Check Answer Softmax For the last lesson of this chapter, we will go over the softmax function, since it will appear again once we dive into the attention blocks. The idea is that if we want a sequence of numbers to serve as a probability", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 3150, "end": 3349}, "timestamp": "2025-04-23T14:59:37.383378", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_3150", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "rows 320,000,000 tokens 10,000 rows Check Answer Softmax For the last lesson of this chapter, we will go over the softmax function, since it will appear again once we dive into the attention blocks. The idea is that if we want a sequence of numbers to serve as a probability distribution, say a distribution over all possible next words, all the values should be between 0 and 1 and should all add up to be 1. However, in deep learning, where so much of what we do looks like a matrix-vector products, the outputs we get by default won\u2019t abide by this at all. The values are often negative or sometimes greater than 1, and they almost certainly don\u2019t all add up to 1. Softmax turns an arbitrary list of numbers into a valid distribution, in such a way that the largest values end up closest to 1, and the smaller values end up closer to 0. The way it works is to first raise e to the power of each number, which gives a bunch of positive values. Then you take the sum of all these new terms and divide each term by that sum, which normalizes it into", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 3300, "end": 3499}, "timestamp": "2025-04-23T14:59:37.383382", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_3300", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "smaller values end up closer to 0. The way it works is to first raise e to the power of each number, which gives a bunch of positive values. Then you take the sum of all these new terms and divide each term by that sum, which normalizes it into a list that adds up to 1. The reason for calling it softmax is that instead of simply pulling out the biggest value, it produces a distribution that gives weight to all the relatively large values, commensurate with how large they are. If one entry in the input is much bigger than the rest, the corresponding output will be very close to 1, so sampling from the distribution is likely the same as just choosing the maximizing index from the input. In some cases, such as when ChatGPT is using this distribution to sample each next word, we can add a little extra spice into this softmax function with a constant T inserted into all these exponents. We call it \"temperature\" since it vaguely resembles the role of temperature in certain thermodynamics equations. The effect of this constant is that when T is larger, more weight is given to lower", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 3450, "end": 3649}, "timestamp": "2025-04-23T14:59:37.383385", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_3450", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "a little extra spice into this softmax function with a constant T inserted into all these exponents. We call it \"temperature\" since it vaguely resembles the role of temperature in certain thermodynamics equations. The effect of this constant is that when T is larger, more weight is given to lower values, making the distribution more uniform. When T is smaller, the larger values will dominate the distribution, and in the extreme setting when T is equal to 0, all the weight goes to the maximum value. Here's another bit of jargon: In the same way that we might call the components of the output of this function probabilities , people often refer to the components of the input as logits . So when you feed in some text, have the word embeddings flow through the network, and do this final multiplication by the unembedding matrix, machine learning people would refer to the components of that raw unnormalized output as the logits for the next word prediction. And That's The Overall Structure Our goal for this chapter was to lay the foundations for understanding the attention mechanism. If we have a strong intuition for word embeddings , softmax , how dot", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 3600, "end": 3799}, "timestamp": "2025-04-23T14:59:37.383389", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_3600", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "to the components of that raw unnormalized output as the logits for the next word prediction. And That's The Overall Structure Our goal for this chapter was to lay the foundations for understanding the attention mechanism. If we have a strong intuition for word embeddings , softmax , how dot products measure similarity, and the underlying premise that most calculations look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, one of the keystone pieces of the whole modern boom in AI that we will explore in the next chapter, should be relatively smooth. Enjoy this lesson? Consider sharing it. Twitter Reddit Facebook Want more math in your life? Notice a mistake? Submit a correction on GitHub Large Language Models explained briefly Visualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning Table of Contents But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5 What is a GPT model? What exactly is a Transformer? Tokens Attention Block Multilayer Perceptron(Feed-Forward Layer) Premise of Deep Learning Embedding Embedding Matrix Direction Dot Product Beyond words Unembedding Unembedding Matrix Softmax And That's The Overall Structure Thanks Thanks Special thanks to those below for supporting", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 3750, "end": 3949}, "timestamp": "2025-04-23T14:59:37.383394", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_3750", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "Deep learning, chapter 5 What is a GPT model? What exactly is a Transformer? Tokens Attention Block Multilayer Perceptron(Feed-Forward Layer) Premise of Deep Learning Embedding Embedding Matrix Direction Dot Product Beyond words Unembedding Unembedding Matrix Softmax And That's The Overall Structure Thanks Thanks Special thanks to those below for supporting the original video behind this post, and to current patrons for funding ongoing projects. If you find these lessons valuable, consider joining . Dwaipayan D Mauricio S\u00e1nchez-Bella Jon Adams John Haley Jean-Manuel Izaret Atlas Fellowship Rod S Andrew Busey Shardul Heda Karim Safin Ali Yahya Ivan Rami Maalouf Evan Miyazono Constantine Goltsev Tyler Herrmann Dave Kester Joe Pregracke Matt Parlmer Oleksandr Kuvshynov Krishanu Sankar Lambda GPU Workstations Ronnie Cheng Andre Au Mutual Information Mike Dussault Patch Kessler Cy 'kkm' K'Nelson Adam D\u0159\u00ednek otavio good Jeremy Smith Alex Hackman Chris Seaby Eryq Ouithaqueue Scott Gray Frank R. Brown, Jr. Cristian Amitroaie Paul Pluzhnikov Michael Dunworth Siddhesh Vichare Andrew Villagran Randy True Christian Bro\u00df namewithheld Bpendragon emptymachine Filip Rolenec Sergey Ovchinnikov Ada Cohen abdulaziz Corey Ogburn Jed Yeiser Bradley Pirtle Craig Schwandt Jonathon Krall Randy C. Will dave nicponski Tal Einav Brian King Steve Muench Sujay Jayakar Delton Ding Barry Fam Alex", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 3900, "end": 4099}, "timestamp": "2025-04-23T14:59:37.383400", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_3900", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "Paul Pluzhnikov Michael Dunworth Siddhesh Vichare Andrew Villagran Randy True Christian Bro\u00df namewithheld Bpendragon emptymachine Filip Rolenec Sergey Ovchinnikov Ada Cohen abdulaziz Corey Ogburn Jed Yeiser Bradley Pirtle Craig Schwandt Jonathon Krall Randy C. Will dave nicponski Tal Einav Brian King Steve Muench Sujay Jayakar Delton Ding Barry Fam Alex Thomas Peter Berntsen Raymond Fowkes JosephG Sergey Korkhov Benjamin Bailey Holger Flier Michael Kokosenski David J Wu John Luttig Nick Valverde Majid Alfifi MR. FANTASTIC Alon Amit Lee Burnette Aljoscha Schulze Marshall McQuillen you say long names are tricky but all I hear is sempai noticed me Ripta Pasay Killian McGuinness Sundar Subbarayan Mike Dour Rob Granieri John C. Vesey Mohammad Hosseini Eduardo M Chumbes Peter Mcinerney Jeff R Marco Dan Martin Oliver Steele Mara Cating-Subramanian Boris Veselinovich Ryan Atallah Vikram Paralkar Vignan Velivela Joshua Ouellette Justin Chandler Zachariah Rosenberg Dan Kinch Andrea Di Biagio Lee Redden Garbanarba Keith Tyson Paul Wolfgang David Bar-On Aman Karunakaran nothings Tyler Parcell Rob Merl Smarter Every Day Aravind C V Andrew Wyld Maksim Stepanenko Brian Cloutier Rebecca Lin Hayden Christensen Octavian Voicu bruce oberg Jimmy Yang Gabriele Siino Daniel Rolandsgard Kjellevold Deep Kalra Vignesh Valliappan Mohsen Hejrati Jamie Winterton \u0410\u043d\u0442\u043e\u043d \u041a\u043e\u0447\u043a\u043e\u0432 Chris Connett", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 4050, "end": 4249}, "timestamp": "2025-04-23T14:59:37.383404", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_4050", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "Paul Wolfgang David Bar-On Aman Karunakaran nothings Tyler Parcell Rob Merl Smarter Every Day Aravind C V Andrew Wyld Maksim Stepanenko Brian Cloutier Rebecca Lin Hayden Christensen Octavian Voicu bruce oberg Jimmy Yang Gabriele Siino Daniel Rolandsgard Kjellevold Deep Kalra Vignesh Valliappan Mohsen Hejrati Jamie Winterton \u0410\u043d\u0442\u043e\u043d \u041a\u043e\u0447\u043a\u043e\u0432 Chris Connett Albin Egasse John Zelinka Dominik Wagner Person Jonathan Gregory Hopper A Zook C Carothers Pradeep Gollakota Vai-Lam Mui D. Sivakumar Maxim Kuzmich Molly Mackinlay Henry Reich The 1111 Code Gerardo Ubaghs Jeremy Lukas Biewald d5b Perry Taga Peter Freese Britt Selvitelle Jacob Harmon Jeff Straathof Kenneth Larsen Eugene Pakhomov Mukul Ram Aaron Binns Ashwany Rayu George Maged Botros Jonathan Whitmore Alvin Khaled Eric Younge Yushi Wang Martin Mauersberg John Le Kristoff Kiefer Jan-Hendrik Prinz Arkajyoti Misra Steve Huynh Ved Deshpande Ben Delo Max Welz Dan Davison David Johnston Steven Siddals P\u0101vils Jurj\u0101ns Kevin Jaewon Jung lardysoft John Tristan Saldanha Andrey Chursin Jonathan M. Jeff Butterworth Carlos Iriarte leafyleaf Stefan Korntreff Dallas De Atley Nate Glissmeyer Philipp Legner Donal Botkin Arun Kulshreshtha gregostras Jorge Matamoros David Gow Alex Sidorovsky Marek Gluszczuk Yoon Suk Oh Avi Bryant Pavel Dubov Anthony Eufemio Vignesh Ganapathi Subramanian Ubiquity Ventures Hitoshi Yamauchi Jeff Linse Alexandru Irimiea", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 4200, "end": 4399}, "timestamp": "2025-04-23T14:59:37.383408", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_4200", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "Chursin Jonathan M. Jeff Butterworth Carlos Iriarte leafyleaf Stefan Korntreff Dallas De Atley Nate Glissmeyer Philipp Legner Donal Botkin Arun Kulshreshtha gregostras Jorge Matamoros David Gow Alex Sidorovsky Marek Gluszczuk Yoon Suk Oh Avi Bryant Pavel Dubov Anthony Eufemio Vignesh Ganapathi Subramanian Ubiquity Ventures Hitoshi Yamauchi Jeff Linse Alexandru Irimiea Alexis Olson Alan Stein Adam Cedrone Jeff Dodds Orb Li Nipun Ramakrishnan rehmi post Dan Lawrence Prathap Sridharan Christian Mercat Vijay Bigyan Bhar Axel Ericsson John Camp Sean Chittenden .chance Bill Six Max Li Russel Simmons James Golab Arthur Zey Sam Stromwall Mano Prime Jack Thull Cooper Jones Julian Parmar Carolus Reinecke Yurii Monastyrshyn Karma Shi Terry Hayes Infinite Flite Mike Jerris Heaton Nate Pinsky Joseph John Cox Taras Bobrovytsky Matthew Bouchard Charles Southerland John McClaskey Pierre Lancien Eric Koslow Jonathan Wilson Maxim Nitsche Eddy Lazzarin Jeremy Cole Dan Herbatschek Max Anderson Tyler Veness David Barker Daniel Badgio Michael Liuzzi MarkM Ron Capelli J. Dmitri Gallow Yana Chernobilsky aeroeng15 Sinan Taifour Chris Mabee Xierumeng Jason Hise Konstantin Belotserkovets Brian Staroselsky Vince Gabor Scott Gorlick Burt Humburg Tanmayan Pande Linda Xie Matthew Piziak Josh Wiley Janak Ramakrishnan Beckett Madden-Woods Timothy Chklovski Patrick Lucas Ero Carrera \u041e\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u0413\u043e\u0440\u043b\u0435\u043d\u043a\u043e Juan Benet Barry McLaurin", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 4350, "end": 4549}, "timestamp": "2025-04-23T14:59:37.383412", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_4350", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "MarkM Ron Capelli J. Dmitri Gallow Yana Chernobilsky aeroeng15 Sinan Taifour Chris Mabee Xierumeng Jason Hise Konstantin Belotserkovets Brian Staroselsky Vince Gabor Scott Gorlick Burt Humburg Tanmayan Pande Linda Xie Matthew Piziak Josh Wiley Janak Ramakrishnan Beckett Madden-Woods Timothy Chklovski Patrick Lucas Ero Carrera \u041e\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u0413\u043e\u0440\u043b\u0435\u043d\u043a\u043e Juan Benet Barry McLaurin Nick Show More \u00a9 2025 Grant Sanderson", "url": "https://www.3blue1brown.com/lessons/gpt", "position": {"start": 4500, "end": 4556}, "timestamp": "2025-04-23T14:59:37.383415", "chunk_id": "https_www_3blue1brown_com_lessons_gpt_4500", "title": "3Blue1Brown - But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5"}, {"text": "Claude Code tutorials - Anthropic Anthropic home page English Search... \u2318K Research News Go to claude.ai Go to claude.ai Search... Navigation Claude Code Claude Code tutorials Welcome User Guides API Reference Prompt Library Release Notes Developer Console Developer Discord Support Get started Overview Initial setup Intro to Claude Learn about Claude Use cases Models & pricing Security and compliance Build with Claude Define success criteria Develop test cases Context windows Vision Prompt engineering Extended thinking Multilingual support Tool use (function calling) Prompt caching PDF support Citations Token counting Batch processing Embeddings Agents and tools Claude Code Overview Claude Code tutorials Troubleshooting Computer use (beta) Model Context Protocol (MCP) Google Sheets add-on Test and evaluate Strengthen guardrails Using the Evaluation Tool Administration Admin API Resources Glossary Model deprecations System status Claude 3 model card Claude 3.7 system card Anthropic Cookbook Anthropic Courses API features Legal center Anthropic Privacy Policy Claude Code Claude Code tutorials Practical examples and patterns for effectively using Claude Code in your development workflow. This guide provides step-by-step tutorials for common workflows with Claude Code. Each tutorial includes clear instructions, example commands, and best practices to help you get the most from Claude Code. \u200b Table of contents", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 0, "end": 199}, "timestamp": "2025-04-29T22:57:14.706374", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_0", "title": "Claude Code tutorials - Anthropic"}, {"text": "Claude Code tutorials Practical examples and patterns for effectively using Claude Code in your development workflow. This guide provides step-by-step tutorials for common workflows with Claude Code. Each tutorial includes clear instructions, example commands, and best practices to help you get the most from Claude Code. \u200b Table of contents Understand new codebases Fix bugs efficiently Refactor code Work with tests Create pull requests Handle documentation Work with images Use extended thinking Set up project memory Set up Model Context Protocol (MCP) Use Claude as a unix-style utility Create custom slash commands Run parallel Claude Code sessions with Git worktrees \u200b Understand new codebases \u200b Get a quick codebase overview When to use: You\u2019ve just joined a new project and need to understand its structure quickly. 1 Navigate to the project root directory Copy cd /path/to/project 2 Start Claude Code Copy claude 3 Ask for a high-level overview Copy > give me an overview of this codebase 4 Dive deeper into specific components Copy > explain the main architecture patterns used here Copy > what are the key data models? Copy > how is authentication handled? Tips: Start with broad questions, then narrow down to specific areas Ask about coding", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 150, "end": 349}, "timestamp": "2025-04-29T22:57:14.706392", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_150", "title": "Claude Code tutorials - Anthropic"}, {"text": "give me an overview of this codebase 4 Dive deeper into specific components Copy > explain the main architecture patterns used here Copy > what are the key data models? Copy > how is authentication handled? Tips: Start with broad questions, then narrow down to specific areas Ask about coding conventions and patterns used in the project Request a glossary of project-specific terms \u200b Find relevant code When to use: You need to locate code related to a specific feature or functionality. 1 Ask Claude to find relevant files Copy > find the files that handle user authentication 2 Get context on how components interact Copy > how do these authentication files work together? 3 Understand the execution flow Copy > trace the login process from front-end to database Tips: Be specific about what you\u2019re looking for Use domain language from the project \u200b Fix bugs efficiently \u200b Diagnose error messages When to use: You\u2019ve encountered an error message and need to find and fix its source. 1 Share the error with Claude Copy > I'm seeing an error when I run npm test 2 Ask for fix recommendations Copy > suggest a few ways to fix the @ts-ignore in", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 300, "end": 499}, "timestamp": "2025-04-29T22:57:14.706400", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_300", "title": "Claude Code tutorials - Anthropic"}, {"text": "messages When to use: You\u2019ve encountered an error message and need to find and fix its source. 1 Share the error with Claude Copy > I'm seeing an error when I run npm test 2 Ask for fix recommendations Copy > suggest a few ways to fix the @ts-ignore in user.ts 3 Apply the fix Copy > update user.ts to add the null check you suggested Tips: Tell Claude the command to reproduce the issue and get a stack trace Mention any steps to reproduce the error Let Claude know if the error is intermittent or consistent \u200b Refactor code \u200b Modernize legacy code When to use: You need to update old code to use modern patterns and practices. 1 Identify legacy code for refactoring Copy > find deprecated API usage in our codebase 2 Get refactoring recommendations Copy > suggest how to refactor utils.js to use modern JavaScript features 3 Apply the changes safely Copy > refactor utils.js to use ES2024 features while maintaining the same behavior 4 Verify the refactoring Copy > run tests for the refactored code Tips: Ask Claude to explain the benefits of the modern approach Request that changes maintain backward compatibility when needed Do", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 450, "end": 649}, "timestamp": "2025-04-29T22:57:14.706406", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_450", "title": "Claude Code tutorials - Anthropic"}, {"text": "3 Apply the changes safely Copy > refactor utils.js to use ES2024 features while maintaining the same behavior 4 Verify the refactoring Copy > run tests for the refactored code Tips: Ask Claude to explain the benefits of the modern approach Request that changes maintain backward compatibility when needed Do refactoring in small, testable increments \u200b Work with tests \u200b Add test coverage When to use: You need to add tests for uncovered code. 1 Identify untested code Copy > find functions in NotificationsService.swift that are not covered by tests 2 Generate test scaffolding Copy > add tests for the notification service 3 Add meaningful test cases Copy > add test cases for edge conditions in the notification service 4 Run and verify tests Copy > run the new tests and fix any failures Tips: Ask for tests that cover edge cases and error conditions Request both unit and integration tests when appropriate Have Claude explain the testing strategy \u200b Create pull requests \u200b Generate comprehensive PRs When to use: You need to create a well-documented pull request for your changes. 1 Summarize your changes Copy > summarize the changes I've made to the authentication module 2 Generate a PR", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 600, "end": 799}, "timestamp": "2025-04-29T22:57:14.706411", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_600", "title": "Claude Code tutorials - Anthropic"}, {"text": "tests when appropriate Have Claude explain the testing strategy \u200b Create pull requests \u200b Generate comprehensive PRs When to use: You need to create a well-documented pull request for your changes. 1 Summarize your changes Copy > summarize the changes I've made to the authentication module 2 Generate a PR with Claude Copy > create a pr 3 Review and refine Copy > enhance the PR description with more context about the security improvements 4 Add testing details Copy > add information about how these changes were tested Tips: Ask Claude directly to make a PR for you Review Claude\u2019s generated PR before submitting Ask Claude to highlight potential risks or considerations \u200b Handle documentation \u200b Generate code documentation When to use: You need to add or update documentation for your code. 1 Identify undocumented code Copy > find functions without proper JSDoc comments in the auth module 2 Generate documentation Copy > add JSDoc comments to the undocumented functions in auth.js 3 Review and enhance Copy > improve the generated documentation with more context and examples 4 Verify documentation Copy > check if the documentation follows our project standards Tips: Specify the documentation style you want (JSDoc, docstrings, etc.)", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 750, "end": 949}, "timestamp": "2025-04-29T22:57:14.706416", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_750", "title": "Claude Code tutorials - Anthropic"}, {"text": "documentation Copy > add JSDoc comments to the undocumented functions in auth.js 3 Review and enhance Copy > improve the generated documentation with more context and examples 4 Verify documentation Copy > check if the documentation follows our project standards Tips: Specify the documentation style you want (JSDoc, docstrings, etc.) Ask for examples in the documentation Request documentation for public APIs, interfaces, and complex logic \u200b Work with images \u200b Analyze images and screenshots When to use: You need to work with images in your codebase or get Claude\u2019s help analyzing image content. 1 Add an image to the conversation You can use any of these methods: Copy # 1. Drag and drop an image into the Claude Code window # 2. Copy an image and paste it into the CLI with ctrl+v # 3. Provide an image path claude \"Analyze this image: /path/to/your/image.png\" 2 Ask Claude to analyze the image Copy > What does this image show? > Describe the UI elements in this screenshot > Are there any problematic elements in this diagram? 3 Use images for context Copy > Here's a screenshot of the error. What's causing it? > This is our current database schema. How should", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 900, "end": 1099}, "timestamp": "2025-04-29T22:57:14.706421", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_900", "title": "Claude Code tutorials - Anthropic"}, {"text": "image Copy > What does this image show? > Describe the UI elements in this screenshot > Are there any problematic elements in this diagram? 3 Use images for context Copy > Here's a screenshot of the error. What's causing it? > This is our current database schema. How should we modify it for the new feature? 4 Get code suggestions from visual content Copy > Generate CSS to match this design mockup > What HTML structure would recreate this component? Tips: Use images when text descriptions would be unclear or cumbersome Include screenshots of errors, UI designs, or diagrams for better context You can work with multiple images in a conversation Image analysis works with diagrams, screenshots, mockups, and more \u200b Use extended thinking \u200b Leverage Claude\u2019s extended thinking for complex tasks When to use: When working on complex architectural decisions, challenging bugs, or planning multi-step implementations that require deep reasoning. 1 Provide context and ask Claude to think Copy > I need to implement a new authentication system using OAuth2 for our API. Think deeply about the best approach for implementing this in our codebase. Claude will gather relevant information from your codebase and use extended thinking, which", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 1050, "end": 1249}, "timestamp": "2025-04-29T22:57:14.706426", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_1050", "title": "Claude Code tutorials - Anthropic"}, {"text": "deep reasoning. 1 Provide context and ask Claude to think Copy > I need to implement a new authentication system using OAuth2 for our API. Think deeply about the best approach for implementing this in our codebase. Claude will gather relevant information from your codebase and use extended thinking, which will be visible in the interface. 2 Refine the thinking with follow-up prompts Copy > think about potential security vulnerabilities in this approach > think harder about edge cases we should handle Tips to get the most value out of extended thinking: Extended thinking is most valuable for complex tasks such as: Planning complex architectural changes Debugging intricate issues Creating implementation plans for new features Understanding complex codebases Evaluating tradeoffs between different approaches The way you prompt for thinking results in varying levels of thinking depth: \u201cthink\u201d triggers basic extended thinking intensifying phrases such as \u201cthink more\u201d, \u201cthink a lot\u201d, \u201cthink harder\u201d, or \u201cthink longer\u201d triggers deeper thinking For more extended thinking prompting tips, see Extended thinking tips . Claude will display its thinking process as italic gray text above the response. \u200b Set up project memory \u200b Create an effective CLAUDE.md file When to use: You want to set", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 1200, "end": 1399}, "timestamp": "2025-04-29T22:57:14.706432", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_1200", "title": "Claude Code tutorials - Anthropic"}, {"text": "\u201cthink harder\u201d, or \u201cthink longer\u201d triggers deeper thinking For more extended thinking prompting tips, see Extended thinking tips . Claude will display its thinking process as italic gray text above the response. \u200b Set up project memory \u200b Create an effective CLAUDE.md file When to use: You want to set up a CLAUDE.md file to store important project information, conventions, and frequently used commands. 1 Bootstrap a CLAUDE.md for your codebase Copy > /init Tips: Include frequently used commands (build, test, lint) to avoid repeated searches Document code style preferences and naming conventions Add important architectural patterns specific to your project You can add CLAUDE.md files to any of: The folder you run Claude in: Automatically added to conversations you start in that folder Child directories: Claude pulls these in on demand ~/.claude/CLAUDE.md : User-specific preferences that you don\u2019t want to check into source control \u200b Set up Model Context Protocol (MCP) Model Context Protocol (MCP) is an open protocol that enables LLMs to access external tools and data sources. For more details, see the MCP documentation . Use third party MCP servers at your own risk. Make sure you trust the MCP servers, and be especially careful when using", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 1350, "end": 1549}, "timestamp": "2025-04-29T22:57:14.706438", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_1350", "title": "Claude Code tutorials - Anthropic"}, {"text": "Protocol (MCP) Model Context Protocol (MCP) is an open protocol that enables LLMs to access external tools and data sources. For more details, see the MCP documentation . Use third party MCP servers at your own risk. Make sure you trust the MCP servers, and be especially careful when using MCP servers that talk to the internet, as these can expose you to prompt injection risk. \u200b Configure MCP servers When to use: You want to enhance Claude\u2019s capabilities by connecting it to specialized tools and external servers using the Model Context Protocol. 1 Add an MCP Stdio Server Copy # Basic syntax claude mcp add < name > < command > [ args .. . ] # Example: Adding a local server claude mcp add my-server -e API_KEY = 123 -- /path/to/server arg1 arg2 2 Add an MCP SSE Server Copy # Basic syntax claude mcp add --transport sse < name > < url > # Example: Adding an SSE server claude mcp add --transport sse sse-server https://example.com/sse-endpoint 3 Manage your MCP servers Copy # List all configured servers claude mcp list # Get details for a specific server claude mcp get my-server # Remove a server claude mcp", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 1500, "end": 1699}, "timestamp": "2025-04-29T22:57:14.706444", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_1500", "title": "Claude Code tutorials - Anthropic"}, {"text": "< name > < url > # Example: Adding an SSE server claude mcp add --transport sse sse-server https://example.com/sse-endpoint 3 Manage your MCP servers Copy # List all configured servers claude mcp list # Get details for a specific server claude mcp get my-server # Remove a server claude mcp remove my-server Tips: Use the -s or --scope flag to specify where the configuration is stored: local (default): Available only to you in the current project (was called project in older versions) project : Shared with everyone in the project via .mcp.json file user : Available to you across all projects (was called global in older versions) Set environment variables with -e or --env flags (e.g., -e KEY=value ) Configure MCP server startup timeout using the MCP_TIMEOUT environment variable (e.g., MCP_TIMEOUT=10000 claude sets a 10-second timeout) Check MCP server status any time using the /mcp command within Claude Code MCP follows a client-server architecture where Claude Code (the client) can connect to multiple specialized servers \u200b Understanding MCP server scopes When to use: You want to understand how different MCP scopes work and how to share servers with your team. 1 Local-scoped MCP servers The default scope ( local )", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 1650, "end": 1849}, "timestamp": "2025-04-29T22:57:14.706449", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_1650", "title": "Claude Code tutorials - Anthropic"}, {"text": "follows a client-server architecture where Claude Code (the client) can connect to multiple specialized servers \u200b Understanding MCP server scopes When to use: You want to understand how different MCP scopes work and how to share servers with your team. 1 Local-scoped MCP servers The default scope ( local ) stores MCP server configurations in your project-specific user settings. These servers are only available to you while working in the current project. Copy # Add a local-scoped server (default) claude mcp add my-private-server /path/to/server # Explicitly specify local scope claude mcp add my-private-server -s local /path/to/server 2 Project-scoped MCP servers (.mcp.json) Project-scoped servers are stored in a .mcp.json file at the root of your project. This file should be checked into version control to share servers with your team. Copy # Add a project-scoped server claude mcp add shared-server -s project /path/to/server This creates or updates a .mcp.json file with the following structure: Copy { \"mcpServers\" : { \"shared-server\" : { \"command\" : \"/path/to/server\" , \"args\" : [ ] , \"env\" : { } } } } 3 User-scoped MCP servers User-scoped servers are available to you across all projects on your machine, and are private to you. Copy #", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 1800, "end": 1999}, "timestamp": "2025-04-29T22:57:14.706454", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_1800", "title": "Claude Code tutorials - Anthropic"}, {"text": "the following structure: Copy { \"mcpServers\" : { \"shared-server\" : { \"command\" : \"/path/to/server\" , \"args\" : [ ] , \"env\" : { } } } } 3 User-scoped MCP servers User-scoped servers are available to you across all projects on your machine, and are private to you. Copy # Add a user server claude mcp add my-user-server -s user /path/to/server Tips: Local-scoped servers take precedence over project-scoped and user-scoped servers with the same name Project-scoped servers (in .mcp.json ) take precedence over user-scoped servers with the same name Before using project-scoped servers from .mcp.json , Claude Code will prompt you to approve them for security The .mcp.json file is intended to be checked into version control to share MCP servers with your team Project-scoped servers make it easy to ensure everyone on your team has access to the same MCP tools If you need to reset your choices for which project-scoped servers are enabled or disabled, use the claude mcp reset-project-choices command \u200b Connect to a Postgres MCP server When to use: You want to give Claude read-only access to a PostgreSQL database for querying and schema inspection. 1 Add the Postgres MCP server Copy claude mcp add postgres-server", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 1950, "end": 2149}, "timestamp": "2025-04-29T22:57:14.706459", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_1950", "title": "Claude Code tutorials - Anthropic"}, {"text": "which project-scoped servers are enabled or disabled, use the claude mcp reset-project-choices command \u200b Connect to a Postgres MCP server When to use: You want to give Claude read-only access to a PostgreSQL database for querying and schema inspection. 1 Add the Postgres MCP server Copy claude mcp add postgres-server /path/to/postgres-mcp-server --connection-string \"postgresql://user:pass@localhost:5432/mydb\" 2 Query your database with Claude Copy # In your Claude session, you can ask about your database > describe the schema of our users table > what are the most recent orders in the system? > show me the relationship between customers and invoices Tips: The Postgres MCP server provides read-only access for safety Claude can help you explore database structure and run analytical queries You can use this to quickly understand database schemas in unfamiliar projects Make sure your connection string uses appropriate credentials with minimum required permissions \u200b Add MCP servers from JSON configuration When to use: You have a JSON configuration for a single MCP server that you want to add to Claude Code. 1 Add an MCP server from JSON Copy # Basic syntax claude mcp add-json < name > '<json>' # Example: Adding a stdio server with JSON configuration claude mcp", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 2100, "end": 2299}, "timestamp": "2025-04-29T22:57:14.706464", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_2100", "title": "Claude Code tutorials - Anthropic"}, {"text": "When to use: You have a JSON configuration for a single MCP server that you want to add to Claude Code. 1 Add an MCP server from JSON Copy # Basic syntax claude mcp add-json < name > '<json>' # Example: Adding a stdio server with JSON configuration claude mcp add-json weather-api '{\"type\":\"stdio\",\"command\":\"/path/to/weather-cli\",\"args\":[\"--api-key\",\"abc123\"],\"env\":{\"CACHE_DIR\":\"/tmp\"}}' 2 Verify the server was added Copy claude mcp get weather-api Tips: Make sure the JSON is properly escaped in your shell The JSON must conform to the MCP server configuration schema You can use -s global to add the server to your global configuration instead of the project-specific one \u200b Import MCP servers from Claude Desktop When to use: You have already configured MCP servers in Claude Desktop and want to use the same servers in Claude Code without manually reconfiguring them. 1 Import servers from Claude Desktop Copy # Basic syntax claude mcp add-from-claude-desktop 2 Select which servers to import After running the command, you\u2019ll see an interactive dialog that allows you to select which servers you want to import. 3 Verify the servers were imported Copy claude mcp list Tips: This feature only works on macOS and Windows Subsystem for Linux (WSL) It reads", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 2250, "end": 2449}, "timestamp": "2025-04-29T22:57:14.706469", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_2250", "title": "Claude Code tutorials - Anthropic"}, {"text": "Select which servers to import After running the command, you\u2019ll see an interactive dialog that allows you to select which servers you want to import. 3 Verify the servers were imported Copy claude mcp list Tips: This feature only works on macOS and Windows Subsystem for Linux (WSL) It reads the Claude Desktop configuration file from its standard location on those platforms Use the -s global flag to add servers to your global configuration Imported servers will have the same names as in Claude Desktop If servers with the same names already exist, they will get a numerical suffix (e.g., server_1 ) \u200b Use Claude Code as an MCP server When to use: You want to use Claude Code itself as an MCP server that other applications can connect to, providing them with Claude\u2019s tools and capabilities. 1 Start Claude as an MCP server Copy # Basic syntax claude mcp serve 2 Connect from another application You can connect to Claude Code MCP server from any MCP client, such as Claude Desktop. If you\u2019re using Claude Desktop, you can add the Claude Code MCP server using this configuration: Copy { \"command\" : \"claude\" , \"args\" : [ \"mcp\" , \"serve\"", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 2400, "end": 2599}, "timestamp": "2025-04-29T22:57:14.706474", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_2400", "title": "Claude Code tutorials - Anthropic"}, {"text": "serve 2 Connect from another application You can connect to Claude Code MCP server from any MCP client, such as Claude Desktop. If you\u2019re using Claude Desktop, you can add the Claude Code MCP server using this configuration: Copy { \"command\" : \"claude\" , \"args\" : [ \"mcp\" , \"serve\" ] , \"env\" : { } } Tips: The server provides access to Claude\u2019s tools like View, Edit, LS, etc. In Claude Desktop, try asking Claude to read files in a directory, make edits, and more. Note that this MCP server is simply exposing Claude Code\u2019s tools to your MCP client, so your own client is responsible for implementing user confirmation for individual tool calls. \u200b Use Claude as a unix-style utility \u200b Add Claude to your verification process When to use: You want to use Claude Code as a linter or code reviewer. Steps: 1 Add Claude to your build script Copy // package.json { ... \"scripts\" : { ... \"lint:claude\" : \"claude -p 'you are a linter. please look at the changes vs. main and report any issues related to typos. report the filename and line number on one line, and a description of the issue on the", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 2550, "end": 2749}, "timestamp": "2025-04-29T22:57:14.706479", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_2550", "title": "Claude Code tutorials - Anthropic"}, {"text": "build script Copy // package.json { ... \"scripts\" : { ... \"lint:claude\" : \"claude -p 'you are a linter. please look at the changes vs. main and report any issues related to typos. report the filename and line number on one line, and a description of the issue on the second line. do not return any other text.'\" } } \u200b Pipe in, pipe out When to use: You want to pipe data into Claude, and get back data in a structured format. 1 Pipe data through Claude Copy cat build-error.txt | claude -p 'concisely explain the root cause of this build error' > output.txt \u200b Create custom slash commands Claude Code supports custom slash commands that you can create to quickly execute specific prompts or tasks. \u200b Create project-specific commands When to use: You want to create reusable slash commands for your project that all team members can use. 1 Create a commands directory in your project Copy mkdir -p .claude/commands 2 Create a Markdown file for each command Copy echo \"Analyze the performance of this code and suggest three specific optimizations:\" > .claude/commands/optimize.md 3 Use your custom command in Claude Code Copy claude > /project:optimize Tips: Command names", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 2700, "end": 2899}, "timestamp": "2025-04-29T22:57:14.706484", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_2700", "title": "Claude Code tutorials - Anthropic"}, {"text": "1 Create a commands directory in your project Copy mkdir -p .claude/commands 2 Create a Markdown file for each command Copy echo \"Analyze the performance of this code and suggest three specific optimizations:\" > .claude/commands/optimize.md 3 Use your custom command in Claude Code Copy claude > /project:optimize Tips: Command names are derived from the filename (e.g., optimize.md becomes /project:optimize ) You can organize commands in subdirectories (e.g., .claude/commands/frontend/component.md becomes /project:frontend:component ) Project commands are available to everyone who clones the repository The Markdown file content becomes the prompt sent to Claude when the command is invoked \u200b Add command arguments with $ARGUMENTS When to use: You want to create flexible slash commands that can accept additional input from users. 1 Create a command file with the $ARGUMENTS placeholder Copy echo \"Find and fix issue #$ARGUMENTS. Follow these steps: 1 . Understand the issue described in the ticket 2 . Locate the relevant code in our codebase 3 . Implement a solution that addresses the root cause 4 . Add appropriate tests 5 . Prepare a concise PR description\" > .claude/commands/fix-issue.md 2 Use the command with an issue number Copy claude > /project:fix-issue 123 This will replace $ARGUMENTS with \u201c123\u201d in", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 2850, "end": 3049}, "timestamp": "2025-04-29T22:57:14.706489", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_2850", "title": "Claude Code tutorials - Anthropic"}, {"text": "the relevant code in our codebase 3 . Implement a solution that addresses the root cause 4 . Add appropriate tests 5 . Prepare a concise PR description\" > .claude/commands/fix-issue.md 2 Use the command with an issue number Copy claude > /project:fix-issue 123 This will replace $ARGUMENTS with \u201c123\u201d in the prompt. Tips: The $ARGUMENTS placeholder is replaced with any text that follows the command You can position $ARGUMENTS anywhere in your command template Other useful applications: generating test cases for specific functions, creating documentation for components, reviewing code in particular files, or translating content to specified languages \u200b Create personal slash commands When to use: You want to create personal slash commands that work across all your projects. 1 Create a commands directory in your home folder Copy mkdir -p ~/.claude/commands 2 Create a Markdown file for each command Copy echo \"Review this code for security vulnerabilities, focusing on:\" > ~/.claude/commands/security-review.md 3 Use your personal custom command Copy claude > /user:security-review Tips: Personal commands are prefixed with /user: instead of /project: Personal commands are only available to you and not shared with your team Personal commands work across all your projects You can use these for consistent workflows across", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 3000, "end": 3199}, "timestamp": "2025-04-29T22:57:14.706494", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_3000", "title": "Claude Code tutorials - Anthropic"}, {"text": "> ~/.claude/commands/security-review.md 3 Use your personal custom command Copy claude > /user:security-review Tips: Personal commands are prefixed with /user: instead of /project: Personal commands are only available to you and not shared with your team Personal commands work across all your projects You can use these for consistent workflows across different codebases \u200b Run parallel Claude Code sessions with Git worktrees \u200b Use worktrees for isolated coding environments When to use: You need to work on multiple tasks simultaneously with complete code isolation between Claude Code instances. 1 Understand Git worktrees Git worktrees allow you to check out multiple branches from the same repository into separate directories. Each worktree has its own working directory with isolated files, while sharing the same Git history. Learn more in the official Git worktree documentation . 2 Create a new worktree Copy # Create a new worktree with a new branch git worktree add .. /project-feature-a feature-a # Or create a worktree with an existing branch git worktree add .. /project-bugfix bugfix-123 This creates a new directory with a separate working copy of your repository. 3 Run Claude Code in each worktree Copy # Navigate to your worktree cd .. /project-feature-a # Run Claude", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 3150, "end": 3349}, "timestamp": "2025-04-29T22:57:14.706499", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_3150", "title": "Claude Code tutorials - Anthropic"}, {"text": ".. /project-feature-a feature-a # Or create a worktree with an existing branch git worktree add .. /project-bugfix bugfix-123 This creates a new directory with a separate working copy of your repository. 3 Run Claude Code in each worktree Copy # Navigate to your worktree cd .. /project-feature-a # Run Claude Code in this isolated environment claude In another terminal: Copy cd .. /project-bugfix claude 4 Manage your worktrees Copy # List all worktrees git worktree list # Remove a worktree when done git worktree remove .. /project-feature-a Tips: Each worktree has its own independent file state, making it perfect for parallel Claude Code sessions Changes made in one worktree won\u2019t affect others, preventing Claude instances from interfering with each other All worktrees share the same Git history and remote connections For long-running tasks, you can have Claude working in one worktree while you continue development in another Use descriptive directory names to easily identify which task each worktree is for Remember to initialize your development environment in each new worktree according to your project\u2019s setup. Depending on your stack, this might include: JavaScript projects: Running dependency installation ( npm install , yarn ) Python projects: Setting up virtual environments or", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 3300, "end": 3499}, "timestamp": "2025-04-29T22:57:14.706504", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_3300", "title": "Claude Code tutorials - Anthropic"}, {"text": "names to easily identify which task each worktree is for Remember to initialize your development environment in each new worktree according to your project\u2019s setup. Depending on your stack, this might include: JavaScript projects: Running dependency installation ( npm install , yarn ) Python projects: Setting up virtual environments or installing with package managers Other languages: Following your project\u2019s standard setup process \u200b Next steps Claude Code reference implementation Clone our development container reference implementation. Was this page helpful? Yes No Overview Troubleshooting x linkedin On this page Table of contents Understand new codebases Get a quick codebase overview Find relevant code Fix bugs efficiently Diagnose error messages Refactor code Modernize legacy code Work with tests Add test coverage Create pull requests Generate comprehensive PRs Handle documentation Generate code documentation Work with images Analyze images and screenshots Use extended thinking Leverage Claude\u2019s extended thinking for complex tasks Set up project memory Create an effective CLAUDE.md file Set up Model Context Protocol (MCP) Configure MCP servers Understanding MCP server scopes Connect to a Postgres MCP server Add MCP servers from JSON configuration Import MCP servers from Claude Desktop Use Claude Code as an MCP server Use Claude as a unix-style utility", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 3450, "end": 3649}, "timestamp": "2025-04-29T22:57:14.706509", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_3450", "title": "Claude Code tutorials - Anthropic"}, {"text": "memory Create an effective CLAUDE.md file Set up Model Context Protocol (MCP) Configure MCP servers Understanding MCP server scopes Connect to a Postgres MCP server Add MCP servers from JSON configuration Import MCP servers from Claude Desktop Use Claude Code as an MCP server Use Claude as a unix-style utility Add Claude to your verification process Pipe in, pipe out Create custom slash commands Create project-specific commands Add command arguments with $ARGUMENTS Create personal slash commands Run parallel Claude Code sessions with Git worktrees Use worktrees for isolated coding environments Next steps Claude Code tutorials - Anthropic", "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials", "position": {"start": 3600, "end": 3696}, "timestamp": "2025-04-29T22:57:14.706512", "chunk_id": "https_docs_anthropic_com_en_docs_agents-and-tools_claude-code_tutorials_3600", "title": "Claude Code tutorials - Anthropic"}]